{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import os\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "import lxml\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import functools\n",
    "import re\n",
    "from datetime import datetime\n",
    "import spacy\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "CONFIG = {\n",
    "    \"input_file\": \"../data/stackoverflow/stackoverflow_script.parquet\",\n",
    "    \"output_dir\": \"../data/batched_processing\",\n",
    "    \"temp_dir\": \"../data/batched_processing/temp\",\n",
    "    \"log_file\": \"../data/batched_processing/processing_log.json\",\n",
    "    \"batch_size\": 50000,  # Records per batch\n",
    "    \"total_batches\": None,  # Will be calculated\n",
    "    \"completed_batches\": [],  # Will track completed batches\n",
    "}\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs(CONFIG[\"output_dir\"], exist_ok=True)\n",
    "os.makedirs(CONFIG[\"temp_dir\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_and_code_optimized(html_content):\n",
    "    \"\"\"\n",
    "    Efficiently separates text and code blocks from Stack Overflow HTML content\n",
    "    \"\"\"\n",
    "    # Use lxml parser for speed\n",
    "    soup = BeautifulSoup(html_content, \"lxml\")\n",
    "\n",
    "    # Get all code blocks in a single pass\n",
    "    code_elements = soup.find_all(\"code\")\n",
    "    code_blocks = [code.get_text() for code in code_elements]\n",
    "\n",
    "    # Extract text after removing code (more efficient than decomposing in a loop)\n",
    "    for code in code_elements:\n",
    "        code.replace_with(\n",
    "            \" \"\n",
    "        )  # Replace with space instead of removing to maintain text flow\n",
    "\n",
    "    clean_text = soup.get_text(strip=True)\n",
    "\n",
    "    return {\"text\": clean_text, \"code\": code_blocks}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize stemmer once\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Create stopwords set once\n",
    "standard_stops = set(stopwords.words(\"english\"))\n",
    "programming_stops = set()  # Add programming-specific stopwords if needed\n",
    "combined_stops = standard_stops.union(programming_stops)\n",
    "\n",
    "\n",
    "# Cache for stemmed words\n",
    "@functools.lru_cache(maxsize=100000)\n",
    "def cached_stem(word):\n",
    "    return stemmer.stem(word)\n",
    "\n",
    "\n",
    "def preprocess_text(text, remove_stops=True):\n",
    "    \"\"\"Process text content with optimizations\"\"\"\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stop words and apply stemming more efficiently\n",
    "    if remove_stops:\n",
    "        # Use generator expression instead of list comprehension\n",
    "        return \" \".join(\n",
    "            cached_stem(token) for token in tokens if token not in combined_stops\n",
    "        )\n",
    "    else:\n",
    "        # Use generator expression instead of list comprehension\n",
    "        return \" \".join(cached_stem(token) for token in tokens if token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a small, efficient SpaCy model - disable components you don't need\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "# Create stopwords set using spaCy instead of NLTK\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "programming_stops = set()  # Add programming-specific stopwords if needed\n",
    "combined_stops = spacy_stopwords.union(programming_stops)\n",
    "\n",
    "\n",
    "# Cache for lemmatized words\n",
    "@functools.lru_cache(maxsize=100000)\n",
    "def cached_lemmatize(word):\n",
    "    \"\"\"Lemmatize a single word and cache the result\"\"\"\n",
    "    # Process the word with the full pipeline to get lemma\n",
    "    doc = nlp(word)  # Use nlp() instead of nlp.make_doc()\n",
    "    return doc[0].lemma_\n",
    "\n",
    "\n",
    "def preprocess_text_spacy(text, remove_stops=True):\n",
    "    \"\"\"Process text content with SpaCy lemmatization\"\"\"\n",
    "\n",
    "    # Process the entire text through the pipeline\n",
    "    doc = nlp(text)  # Use full nlp pipeline instead of just tokenizer\n",
    "\n",
    "    # Apply lemmatization with stop word removal if needed\n",
    "    if remove_stops:\n",
    "        return \" \".join(\n",
    "            token.lemma_  # Access lemma_ directly from processed tokens\n",
    "            for token in doc\n",
    "            if token.text not in combined_stops and token.text.strip()\n",
    "        )\n",
    "    else:\n",
    "        return \" \".join(\n",
    "            token.lemma_  # Access lemma_ directly from processed tokens\n",
    "            for token in doc\n",
    "            if token.text.strip()\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete batch preprocessing pipeline\n",
    "\n",
    "Use the Polars lazy dataframe implementation to profit from query optimization for large datasets, but still batch it, since it's too much data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch processing setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable Polars to use all available threads for maximum performance\n",
    "pl.Config.set_streaming_chunk_size(CONFIG[\"batch_size\"])\n",
    "\n",
    "\n",
    "# Initialize processing state\n",
    "def initialize_state():\n",
    "    \"\"\"Initialize or load the processing state\"\"\"\n",
    "    if os.path.exists(CONFIG[\"log_file\"]):\n",
    "        with open(CONFIG[\"log_file\"], \"r\") as f:\n",
    "            return json.load(f)\n",
    "    else:\n",
    "        # Count total rows to calculate batches\n",
    "        row_count = (\n",
    "            pl.scan_parquet(CONFIG[\"input_file\"]).select(pl.len()).collect().item()\n",
    "        )\n",
    "        total_batches = (row_count + CONFIG[\"batch_size\"] - 1) // CONFIG[\"batch_size\"]\n",
    "\n",
    "        state = {\n",
    "            \"total_rows\": row_count,\n",
    "            \"total_batches\": total_batches,\n",
    "            \"completed_batches\": [],\n",
    "            \"start_time\": datetime.now().isoformat(),\n",
    "            \"last_processed_time\": None,\n",
    "            \"processing_stats\": {\n",
    "                \"avg_batch_time_seconds\": None,\n",
    "                \"estimated_time_remaining\": None,\n",
    "            },\n",
    "        }\n",
    "        save_state(state)\n",
    "        return state\n",
    "\n",
    "\n",
    "def save_state(state):\n",
    "    \"\"\"Save the current processing state\"\"\"\n",
    "    state[\"last_processed_time\"] = datetime.now().isoformat()\n",
    "    with open(CONFIG[\"log_file\"], \"w\") as f:\n",
    "        json.dump(state, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch_id, state):\n",
    "    \"\"\"Process a single batch of data\"\"\"\n",
    "    if batch_id in state[\"completed_batches\"]:\n",
    "        print(f\"Batch {batch_id} already processed, skipping.\")\n",
    "        return\n",
    "\n",
    "    start_time = time.time()\n",
    "    print(f\"Processing batch {batch_id}/{state['total_batches']}...\")\n",
    "\n",
    "    # Calculate row offset for this batch\n",
    "    offset = batch_id * CONFIG[\"batch_size\"]\n",
    "\n",
    "    # Process the batch\n",
    "    batch_df = (\n",
    "        pl.scan_parquet(CONFIG[\"input_file\"])\n",
    "        .slice(offset, CONFIG[\"batch_size\"])\n",
    "        # 1. Apply filters early to reduce data volume\n",
    "        # .filter(pl.col(\"CreationDate\") > \"2023-12-31\")  # Example early filter\n",
    "        # .limit(100) # Use to test pipeline on a small subset\n",
    "        # 2. Select only needed columns as early as possible\n",
    "        .select(\n",
    "            [\n",
    "                \"Id\",\n",
    "                \"CreationDate\",\n",
    "                \"Score\",\n",
    "                \"ViewCount\",\n",
    "                \"Title\",\n",
    "                \"Body\",\n",
    "                \"Tags\",\n",
    "                \"AnswerCount\",\n",
    "                \"CommentCount\",\n",
    "            ]\n",
    "        )\n",
    "        # 3. Process HTML content in one step\n",
    "        .with_columns(\n",
    "            pl.col(\"Body\")\n",
    "            .map_elements(\n",
    "                extract_text_and_code_optimized,\n",
    "                return_dtype=pl.Struct(\n",
    "                    [pl.Field(\"text\", pl.Utf8), pl.Field(\"code\", pl.List(pl.Utf8))]\n",
    "                ),\n",
    "            )\n",
    "            .alias(\"content\"),\n",
    "            # 4. Process tags in same with_columns operation\n",
    "            pl.col(\"Tags\")\n",
    "            .str.strip_prefix(\"|\")\n",
    "            .str.strip_suffix(\"|\")\n",
    "            .str.split(\"|\")\n",
    "            .alias(\"tag_list\"),\n",
    "        )\n",
    "        # 5. Extract struct fields and derive new columns in one operation\n",
    "        .with_columns(\n",
    "            pl.col(\"content\").struct.field(\"text\").alias(\"clean_text\"),\n",
    "            pl.col(\"content\").struct.field(\"code\").alias(\"code_blocks\"),\n",
    "            pl.col(\"tag_list\").list.len().alias(\"tag_count\"),\n",
    "        )\n",
    "        # 6. Apply text preprocessing only after dropping unnecessary columns\n",
    "        .with_columns(\n",
    "            pl.col(\"code_blocks\").list.join(\"\").str.len_chars().alias(\"code_length\"),\n",
    "            pl.col(\"Body\").str.len_chars().alias(\"body_length\"),\n",
    "            pl.col(\"Title\").str.len_chars().alias(\"title_length\"),\n",
    "        )\n",
    "        .drop([\"Body\", \"Tags\", \"content\"])\n",
    "        # 7. Apply expensive text processing operations last\n",
    "        .with_columns(\n",
    "            pl.col(\"clean_text\")\n",
    "            .str.to_lowercase()\n",
    "            .str.strip_chars()\n",
    "            .replace(\"\", None)\n",
    "            .map_elements(\n",
    "                preprocess_text_spacy,\n",
    "                return_dtype=pl.Utf8(),\n",
    "                skip_nulls=True,\n",
    "            )\n",
    "            .alias(\"processed_text\")\n",
    "        )\n",
    "        # 8. Set execution configuration\n",
    "        .collect(\n",
    "            streaming=True,\n",
    "            no_optimization=False,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Save the processed batch to a parquet file\n",
    "    output_file = f\"{CONFIG['temp_dir']}/batch_{batch_id:04d}.parquet\"\n",
    "    batch_df.write_parquet(output_file)\n",
    "\n",
    "    # Update state\n",
    "    state[\"completed_batches\"].append(batch_id)\n",
    "\n",
    "    # Calculate processing statistics\n",
    "    end_time = time.time()\n",
    "    batch_time = end_time - start_time\n",
    "\n",
    "    # Update average batch time if we have processed batches\n",
    "    if not state[\"processing_stats\"][\"avg_batch_time_seconds\"]:\n",
    "        state[\"processing_stats\"][\"avg_batch_time_seconds\"] = batch_time\n",
    "    else:\n",
    "        # Exponential moving average with alpha=0.3\n",
    "        avg_time = state[\"processing_stats\"][\"avg_batch_time_seconds\"]\n",
    "        state[\"processing_stats\"][\"avg_batch_time_seconds\"] = (\n",
    "            avg_time * 0.7 + batch_time * 0.3\n",
    "        )\n",
    "\n",
    "    # Estimate time remaining\n",
    "    remaining_batches = state[\"total_batches\"] - len(state[\"completed_batches\"])\n",
    "    state[\"processing_stats\"][\"estimated_time_remaining\"] = (\n",
    "        remaining_batches * state[\"processing_stats\"][\"avg_batch_time_seconds\"]\n",
    "    )\n",
    "\n",
    "    # Save updated state\n",
    "    save_state(state)\n",
    "\n",
    "    print(f\"Batch {batch_id} completed in {batch_time:.2f} seconds.\")\n",
    "    print(\n",
    "        f\"Estimated time remaining: {state['processing_stats']['estimated_time_remaining'] / 3600:.2f} hours\"\n",
    "    )\n",
    "\n",
    "    return batch_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_processed_batches(state):\n",
    "    \"\"\"Merge all processed batches into a single parquet file\"\"\"\n",
    "    print(\"Merging processed batches...\")\n",
    "\n",
    "    # List all batch files in order\n",
    "    batch_files = []\n",
    "    for batch_id in range(state[\"total_batches\"]):\n",
    "        file_path = f\"{CONFIG['temp_dir']}/batch_{batch_id:04d}.parquet\"\n",
    "        if os.path.exists(file_path):\n",
    "            batch_files.append(file_path)\n",
    "\n",
    "    if not batch_files:\n",
    "        print(\"No processed batches found to merge.\")\n",
    "        return\n",
    "\n",
    "    # Read and concatenate all batches\n",
    "    merged_df = pl.concat([pl.read_parquet(file) for file in batch_files])\n",
    "\n",
    "    # Save the merged dataframe\n",
    "    output_file = f\"{CONFIG['output_dir']}/stackoverflow_processed_batch.parquet\"\n",
    "    merged_df.write_parquet(output_file)\n",
    "\n",
    "    print(f\"Merged {len(batch_files)} batches into {output_file}\")\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "def run_batch_processing(start_batch=None, end_batch=None):\n",
    "    \"\"\"Run the batch processing pipeline\"\"\"\n",
    "    state = initialize_state()\n",
    "\n",
    "    # Determine batch range to process\n",
    "    if start_batch is None:\n",
    "        start_batch = 0\n",
    "    if end_batch is None:\n",
    "        end_batch = state[\"total_batches\"] - 1\n",
    "\n",
    "    print(f\"Starting batch processing from batch {start_batch} to {end_batch}\")\n",
    "    print(f\"Total rows: {state['total_rows']}, Total batches: {state['total_batches']}\")\n",
    "\n",
    "    try:\n",
    "        for batch_id in range(start_batch, end_batch + 1):\n",
    "            process_batch(batch_id, state)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nProcessing interrupted. Progress has been saved.\")\n",
    "        print(\n",
    "            f\"Completed {len(state['completed_batches'])}/{state['total_batches']} batches.\"\n",
    "        )\n",
    "        return state\n",
    "\n",
    "    print(\"All specified batches completed!\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get processing progress/stats\n",
    "def get_processing_status():\n",
    "    \"\"\"Get current processing status and stats\"\"\"\n",
    "    if os.path.exists(CONFIG[\"log_file\"]):\n",
    "        with open(CONFIG[\"log_file\"], \"r\") as f:\n",
    "            state = json.load(f)\n",
    "\n",
    "        completed = len(state[\"completed_batches\"])\n",
    "        total = state[\"total_batches\"]\n",
    "\n",
    "        print(f\"Progress: {completed}/{total} batches ({completed / total * 100:.1f}%)\")\n",
    "\n",
    "        if state[\"processing_stats\"][\"avg_batch_time_seconds\"]:\n",
    "            avg_time = state[\"processing_stats\"][\"avg_batch_time_seconds\"]\n",
    "            print(f\"Average batch processing time: {avg_time:.2f} seconds\")\n",
    "\n",
    "            remaining_batches = total - completed\n",
    "            est_time = remaining_batches * avg_time\n",
    "            print(\n",
    "                f\"Estimated time remaining: {est_time / 3600:.2f} hours ({est_time / 60:.2f} minutes)\"\n",
    "            )\n",
    "\n",
    "        start_time = datetime.fromisoformat(state[\"start_time\"])\n",
    "        elapsed = (datetime.now() - start_time).total_seconds()\n",
    "        print(f\"Total elapsed time: {elapsed / 3600:.2f} hours\")\n",
    "\n",
    "        return state\n",
    "    else:\n",
    "        print(\n",
    "            \"No processing log found. Run initialize_state() to start new processing.\"\n",
    "        )\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0/26 batches (0.0%)\n",
      "Total elapsed time: 0.17 hours\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_rows': 1288912,\n",
       " 'total_batches': 26,\n",
       " 'completed_batches': [],\n",
       " 'start_time': '2025-03-07T14:50:15.343497',\n",
       " 'last_processed_time': '2025-03-07T14:50:15.343522',\n",
       " 'processing_stats': {'avg_batch_time_seconds': None,\n",
       "  'estimated_time_remaining': None}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize state (do this first)\n",
    "state = initialize_state()\n",
    "\n",
    "# Get current status\n",
    "get_processing_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch processing from batch 0 to 25\n",
      "Total rows: 1288912, Total batches: 26\n",
      "Processing batch 0/26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_129259/1668697141.py:78: DeprecationWarning: The old streaming engine is being deprecated and will soon be replaced by the new streaming engine. Starting Polars version 1.23.0 and until the new streaming engine is released, the old streaming engine may become less usable. For people who rely on the old streaming engine, it is suggested to pin your version to before 1.23.0.\n",
      "\n",
      "More information on the new streaming engine: https://github.com/pola-rs/polars/issues/20947\n",
      "  .collect(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 completed in 411.42 seconds.\n",
      "Estimated time remaining: 2.86 hours\n",
      "Processing batch 1/26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_129259/1668697141.py:78: DeprecationWarning: The old streaming engine is being deprecated and will soon be replaced by the new streaming engine. Starting Polars version 1.23.0 and until the new streaming engine is released, the old streaming engine may become less usable. For people who rely on the old streaming engine, it is suggested to pin your version to before 1.23.0.\n",
      "\n",
      "More information on the new streaming engine: https://github.com/pola-rs/polars/issues/20947\n",
      "  .collect(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 completed in 444.97 seconds.\n",
      "Estimated time remaining: 2.81 hours\n",
      "Processing batch 2/26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_129259/1668697141.py:78: DeprecationWarning: The old streaming engine is being deprecated and will soon be replaced by the new streaming engine. Starting Polars version 1.23.0 and until the new streaming engine is released, the old streaming engine may become less usable. For people who rely on the old streaming engine, it is suggested to pin your version to before 1.23.0.\n",
      "\n",
      "More information on the new streaming engine: https://github.com/pola-rs/polars/issues/20947\n",
      "  .collect(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2 completed in 422.37 seconds.\n",
      "Estimated time remaining: 2.69 hours\n",
      "Processing batch 3/26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_129259/1668697141.py:78: DeprecationWarning: The old streaming engine is being deprecated and will soon be replaced by the new streaming engine. Starting Polars version 1.23.0 and until the new streaming engine is released, the old streaming engine may become less usable. For people who rely on the old streaming engine, it is suggested to pin your version to before 1.23.0.\n",
      "\n",
      "More information on the new streaming engine: https://github.com/pola-rs/polars/issues/20947\n",
      "  .collect(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3 completed in 415.94 seconds.\n",
      "Estimated time remaining: 2.57 hours\n",
      "Processing batch 4/26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_129259/1668697141.py:78: DeprecationWarning: The old streaming engine is being deprecated and will soon be replaced by the new streaming engine. Starting Polars version 1.23.0 and until the new streaming engine is released, the old streaming engine may become less usable. For people who rely on the old streaming engine, it is suggested to pin your version to before 1.23.0.\n",
      "\n",
      "More information on the new streaming engine: https://github.com/pola-rs/polars/issues/20947\n",
      "  .collect(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4 completed in 414.21 seconds.\n",
      "Estimated time remaining: 2.44 hours\n",
      "Processing batch 5/26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_129259/1668697141.py:78: DeprecationWarning: The old streaming engine is being deprecated and will soon be replaced by the new streaming engine. Starting Polars version 1.23.0 and until the new streaming engine is released, the old streaming engine may become less usable. For people who rely on the old streaming engine, it is suggested to pin your version to before 1.23.0.\n",
      "\n",
      "More information on the new streaming engine: https://github.com/pola-rs/polars/issues/20947\n",
      "  .collect(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5 completed in 421.82 seconds.\n",
      "Estimated time remaining: 2.33 hours\n",
      "Processing batch 6/26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_129259/1668697141.py:78: DeprecationWarning: The old streaming engine is being deprecated and will soon be replaced by the new streaming engine. Starting Polars version 1.23.0 and until the new streaming engine is released, the old streaming engine may become less usable. For people who rely on the old streaming engine, it is suggested to pin your version to before 1.23.0.\n",
      "\n",
      "More information on the new streaming engine: https://github.com/pola-rs/polars/issues/20947\n",
      "  .collect(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6 completed in 484.38 seconds.\n",
      "Estimated time remaining: 2.32 hours\n",
      "Processing batch 7/26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_129259/1668697141.py:78: DeprecationWarning: The old streaming engine is being deprecated and will soon be replaced by the new streaming engine. Starting Polars version 1.23.0 and until the new streaming engine is released, the old streaming engine may become less usable. For people who rely on the old streaming engine, it is suggested to pin your version to before 1.23.0.\n",
      "\n",
      "More information on the new streaming engine: https://github.com/pola-rs/polars/issues/20947\n",
      "  .collect(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7 completed in 443.18 seconds.\n",
      "Estimated time remaining: 2.20 hours\n",
      "Processing batch 8/26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_129259/1668697141.py:78: DeprecationWarning: The old streaming engine is being deprecated and will soon be replaced by the new streaming engine. Starting Polars version 1.23.0 and until the new streaming engine is released, the old streaming engine may become less usable. For people who rely on the old streaming engine, it is suggested to pin your version to before 1.23.0.\n",
      "\n",
      "More information on the new streaming engine: https://github.com/pola-rs/polars/issues/20947\n",
      "  .collect(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 8 completed in 466.31 seconds.\n",
      "Estimated time remaining: 2.12 hours\n",
      "Processing batch 9/26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_129259/1668697141.py:78: DeprecationWarning: The old streaming engine is being deprecated and will soon be replaced by the new streaming engine. Starting Polars version 1.23.0 and until the new streaming engine is released, the old streaming engine may become less usable. For people who rely on the old streaming engine, it is suggested to pin your version to before 1.23.0.\n",
      "\n",
      "More information on the new streaming engine: https://github.com/pola-rs/polars/issues/20947\n",
      "  .collect(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 9 completed in 434.80 seconds.\n",
      "Estimated time remaining: 1.97 hours\n",
      "Processing batch 10/26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_129259/1668697141.py:78: DeprecationWarning: The old streaming engine is being deprecated and will soon be replaced by the new streaming engine. Starting Polars version 1.23.0 and until the new streaming engine is released, the old streaming engine may become less usable. For people who rely on the old streaming engine, it is suggested to pin your version to before 1.23.0.\n",
      "\n",
      "More information on the new streaming engine: https://github.com/pola-rs/polars/issues/20947\n",
      "  .collect(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10 completed in 430.41 seconds.\n",
      "Estimated time remaining: 1.83 hours\n",
      "Processing batch 11/26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_129259/1668697141.py:78: DeprecationWarning: The old streaming engine is being deprecated and will soon be replaced by the new streaming engine. Starting Polars version 1.23.0 and until the new streaming engine is released, the old streaming engine may become less usable. For people who rely on the old streaming engine, it is suggested to pin your version to before 1.23.0.\n",
      "\n",
      "More information on the new streaming engine: https://github.com/pola-rs/polars/issues/20947\n",
      "  .collect(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 11 completed in 432.89 seconds.\n",
      "Estimated time remaining: 1.70 hours\n",
      "Processing batch 12/26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_129259/1668697141.py:78: DeprecationWarning: The old streaming engine is being deprecated and will soon be replaced by the new streaming engine. Starting Polars version 1.23.0 and until the new streaming engine is released, the old streaming engine may become less usable. For people who rely on the old streaming engine, it is suggested to pin your version to before 1.23.0.\n",
      "\n",
      "More information on the new streaming engine: https://github.com/pola-rs/polars/issues/20947\n",
      "  .collect(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 12 completed in 516.59 seconds.\n",
      "Estimated time remaining: 1.67 hours\n",
      "Processing batch 13/26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_129259/1668697141.py:78: DeprecationWarning: The old streaming engine is being deprecated and will soon be replaced by the new streaming engine. Starting Polars version 1.23.0 and until the new streaming engine is released, the old streaming engine may become less usable. For people who rely on the old streaming engine, it is suggested to pin your version to before 1.23.0.\n",
      "\n",
      "More information on the new streaming engine: https://github.com/pola-rs/polars/issues/20947\n",
      "  .collect(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 13 completed in 484.40 seconds.\n",
      "Estimated time remaining: 1.56 hours\n",
      "Processing batch 14/26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_129259/1668697141.py:78: DeprecationWarning: The old streaming engine is being deprecated and will soon be replaced by the new streaming engine. Starting Polars version 1.23.0 and until the new streaming engine is released, the old streaming engine may become less usable. For people who rely on the old streaming engine, it is suggested to pin your version to before 1.23.0.\n",
      "\n",
      "More information on the new streaming engine: https://github.com/pola-rs/polars/issues/20947\n",
      "  .collect(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 14 completed in 450.92 seconds.\n",
      "Estimated time remaining: 1.42 hours\n",
      "Processing batch 15/26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_129259/1668697141.py:78: DeprecationWarning: The old streaming engine is being deprecated and will soon be replaced by the new streaming engine. Starting Polars version 1.23.0 and until the new streaming engine is released, the old streaming engine may become less usable. For people who rely on the old streaming engine, it is suggested to pin your version to before 1.23.0.\n",
      "\n",
      "More information on the new streaming engine: https://github.com/pola-rs/polars/issues/20947\n",
      "  .collect(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 15 completed in 443.65 seconds.\n",
      "Estimated time remaining: 1.27 hours\n",
      "Processing batch 16/26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_129259/1668697141.py:78: DeprecationWarning: The old streaming engine is being deprecated and will soon be replaced by the new streaming engine. Starting Polars version 1.23.0 and until the new streaming engine is released, the old streaming engine may become less usable. For people who rely on the old streaming engine, it is suggested to pin your version to before 1.23.0.\n",
      "\n",
      "More information on the new streaming engine: https://github.com/pola-rs/polars/issues/20947\n",
      "  .collect(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 16 completed in 381.07 seconds.\n",
      "Estimated time remaining: 1.09 hours\n",
      "Processing batch 17/26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_129259/1668697141.py:78: DeprecationWarning: The old streaming engine is being deprecated and will soon be replaced by the new streaming engine. Starting Polars version 1.23.0 and until the new streaming engine is released, the old streaming engine may become less usable. For people who rely on the old streaming engine, it is suggested to pin your version to before 1.23.0.\n",
      "\n",
      "More information on the new streaming engine: https://github.com/pola-rs/polars/issues/20947\n",
      "  .collect(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 17 completed in 382.51 seconds.\n",
      "Estimated time remaining: 0.93 hours\n",
      "Processing batch 18/26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_129259/1668697141.py:78: DeprecationWarning: The old streaming engine is being deprecated and will soon be replaced by the new streaming engine. Starting Polars version 1.23.0 and until the new streaming engine is released, the old streaming engine may become less usable. For people who rely on the old streaming engine, it is suggested to pin your version to before 1.23.0.\n",
      "\n",
      "More information on the new streaming engine: https://github.com/pola-rs/polars/issues/20947\n",
      "  .collect(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 18 completed in 410.24 seconds.\n",
      "Estimated time remaining: 0.81 hours\n",
      "Processing batch 19/26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_129259/1668697141.py:78: DeprecationWarning: The old streaming engine is being deprecated and will soon be replaced by the new streaming engine. Starting Polars version 1.23.0 and until the new streaming engine is released, the old streaming engine may become less usable. For people who rely on the old streaming engine, it is suggested to pin your version to before 1.23.0.\n",
      "\n",
      "More information on the new streaming engine: https://github.com/pola-rs/polars/issues/20947\n",
      "  .collect(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 19 completed in 378.89 seconds.\n",
      "Estimated time remaining: 0.68 hours\n",
      "Processing batch 20/26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_129259/1668697141.py:78: DeprecationWarning: The old streaming engine is being deprecated and will soon be replaced by the new streaming engine. Starting Polars version 1.23.0 and until the new streaming engine is released, the old streaming engine may become less usable. For people who rely on the old streaming engine, it is suggested to pin your version to before 1.23.0.\n",
      "\n",
      "More information on the new streaming engine: https://github.com/pola-rs/polars/issues/20947\n",
      "  .collect(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 20 completed in 388.72 seconds.\n",
      "Estimated time remaining: 0.56 hours\n",
      "Processing batch 21/26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_129259/1668697141.py:78: DeprecationWarning: The old streaming engine is being deprecated and will soon be replaced by the new streaming engine. Starting Polars version 1.23.0 and until the new streaming engine is released, the old streaming engine may become less usable. For people who rely on the old streaming engine, it is suggested to pin your version to before 1.23.0.\n",
      "\n",
      "More information on the new streaming engine: https://github.com/pola-rs/polars/issues/20947\n",
      "  .collect(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 21 completed in 393.64 seconds.\n",
      "Estimated time remaining: 0.44 hours\n",
      "Processing batch 22/26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_129259/1668697141.py:78: DeprecationWarning: The old streaming engine is being deprecated and will soon be replaced by the new streaming engine. Starting Polars version 1.23.0 and until the new streaming engine is released, the old streaming engine may become less usable. For people who rely on the old streaming engine, it is suggested to pin your version to before 1.23.0.\n",
      "\n",
      "More information on the new streaming engine: https://github.com/pola-rs/polars/issues/20947\n",
      "  .collect(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 22 completed in 427.14 seconds.\n",
      "Estimated time remaining: 0.34 hours\n",
      "Processing batch 23/26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_129259/1668697141.py:78: DeprecationWarning: The old streaming engine is being deprecated and will soon be replaced by the new streaming engine. Starting Polars version 1.23.0 and until the new streaming engine is released, the old streaming engine may become less usable. For people who rely on the old streaming engine, it is suggested to pin your version to before 1.23.0.\n",
      "\n",
      "More information on the new streaming engine: https://github.com/pola-rs/polars/issues/20947\n",
      "  .collect(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 23 completed in 419.68 seconds.\n",
      "Estimated time remaining: 0.23 hours\n",
      "Processing batch 24/26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_129259/1668697141.py:78: DeprecationWarning: The old streaming engine is being deprecated and will soon be replaced by the new streaming engine. Starting Polars version 1.23.0 and until the new streaming engine is released, the old streaming engine may become less usable. For people who rely on the old streaming engine, it is suggested to pin your version to before 1.23.0.\n",
      "\n",
      "More information on the new streaming engine: https://github.com/pola-rs/polars/issues/20947\n",
      "  .collect(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 24 completed in 416.98 seconds.\n",
      "Estimated time remaining: 0.11 hours\n",
      "Processing batch 25/26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_129259/1668697141.py:78: DeprecationWarning: The old streaming engine is being deprecated and will soon be replaced by the new streaming engine. Starting Polars version 1.23.0 and until the new streaming engine is released, the old streaming engine may become less usable. For people who rely on the old streaming engine, it is suggested to pin your version to before 1.23.0.\n",
      "\n",
      "More information on the new streaming engine: https://github.com/pola-rs/polars/issues/20947\n",
      "  .collect(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 25 completed in 325.26 seconds.\n",
      "Estimated time remaining: 0.00 hours\n",
      "All specified batches completed!\n"
     ]
    }
   ],
   "source": [
    "state = run_batch_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging processed batches...\n",
      "Merged 26 batches into ../data/batched_processing/stackoverflow_processed_batch.parquet\n"
     ]
    }
   ],
   "source": [
    "# When all batches are done, merge them\n",
    "merged_df = merge_processed_batches(initialize_state())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
