{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_parquet(\"../data/tfidf_vocabulary_comparison.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Step 1: Load and prepare your lemmatized text data\n",
    "# Assuming you have a dataframe with columns: 'post_id', 'date', 'lemmatized_text'\n",
    "# Example of loading data (replace with your actual data loading code)\n",
    "df = pl.scan_parquet(\"../data/batched_processing/stackoverflow_processed_batch.parquet\")\n",
    "\n",
    "# Step 2: Define pre and post-treatment periods\n",
    "treatment_date = datetime(2022, 11, 30)\n",
    "df = df.with_columns(\n",
    "    pl.col(\"CreationDate\").cast(pl.Datetime).lt(treatment_date).alias(\"pre_treatment\")\n",
    ")\n",
    "\n",
    "# Step 3: Group texts by pre/post treatment\n",
    "pre_texts = (\n",
    "    df.filter(\n",
    "        (pl.col(\"pre_treatment\") == True) & (pl.col(\"processed_text\").is_not_null())\n",
    "    )\n",
    "    .select(\"processed_text\")\n",
    "    .collect()\n",
    ")\n",
    "post_texts = (\n",
    "    df.filter(\n",
    "        (pl.col(\"pre_treatment\") == False) & (pl.col(\"processed_text\").is_not_null())\n",
    "    )\n",
    "    .select(\"processed_text\")\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "# Convert to lists for scikit-learn\n",
    "pre_texts_list = pre_texts[\"processed_text\"].to_list()\n",
    "post_texts_list = post_texts[\"processed_text\"].to_list()\n",
    "\n",
    "# Step 4: Create TF-IDF matrices for both periods\n",
    "# You can adjust parameters based on your needs\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,  # Consider top 5000 terms\n",
    "    min_df=10,  # Ignore terms that appear in less than 10 documents\n",
    "    max_df=0.7,  # Ignore terms that appear in more than 70% of documents\n",
    "    ngram_range=(1, 2),  # Include unigrams and bigrams\n",
    ")\n",
    "\n",
    "# Step 5: Fit on all documents to ensure consistent vocabulary\n",
    "all_texts = pre_texts_list + post_texts_list\n",
    "tfidf_vectorizer.fit(all_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Transform both pre and post text sets\n",
    "pre_tfidf = tfidf_vectorizer.transform(pre_texts_list)\n",
    "post_tfidf = tfidf_vectorizer.transform(post_texts_list)\n",
    "\n",
    "# Step 7: Extract vocabulary and calculate mean TF-IDF scores for each period\n",
    "vocabulary = tfidf_vectorizer.get_feature_names_out()\n",
    "pre_mean_tfidf = np.asarray(pre_tfidf.mean(axis=0)).flatten()\n",
    "post_mean_tfidf = np.asarray(post_tfidf.mean(axis=0)).flatten()\n",
    "\n",
    "# Step 8: Create a vocabulary table with comparative statistics\n",
    "vocab_table = pl.DataFrame(\n",
    "    {\n",
    "        \"term\": vocabulary,\n",
    "        \"pre_mean_tfidf\": pre_mean_tfidf,\n",
    "        \"post_mean_tfidf\": post_mean_tfidf,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Calculate differences and ratios\n",
    "vocab_table = vocab_table.with_columns(\n",
    "    [\n",
    "        (pl.col(\"post_mean_tfidf\") - pl.col(\"pre_mean_tfidf\")).alias(\"tfidf_diff\"),\n",
    "        (pl.col(\"post_mean_tfidf\") / pl.col(\"pre_mean_tfidf\")).alias(\"tfidf_ratio\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Replace infinity values in ratio with a large number\n",
    "vocab_table = vocab_table.with_columns(\n",
    "    pl.when(pl.col(\"tfidf_ratio\").is_infinite())\n",
    "    .then(1000.0)  # Large value for terms not in pre-treatment\n",
    "    .otherwise(pl.col(\"tfidf_ratio\"))\n",
    "    .alias(\"tfidf_ratio\")\n",
    ")\n",
    "\n",
    "# Step 9: Calculate term frequencies (document occurrence)\n",
    "pre_doc_count = len(pre_texts_list)\n",
    "post_doc_count = len(post_texts_list)\n",
    "\n",
    "pre_term_counts = (pre_tfidf > 0).sum(axis=0).A1  # Count docs containing term\n",
    "post_term_counts = (post_tfidf > 0).sum(axis=0).A1\n",
    "\n",
    "# Add frequency info to the vocabulary table\n",
    "vocab_table = vocab_table.with_columns(\n",
    "    [\n",
    "        (pl.lit(pre_term_counts) / pre_doc_count).alias(\"pre_doc_freq\"),\n",
    "        (pl.lit(post_term_counts) / post_doc_count).alias(\"post_doc_freq\"),\n",
    "        (\n",
    "            (pl.lit(post_term_counts) / post_doc_count)\n",
    "            - (pl.lit(pre_term_counts) / pre_doc_count)\n",
    "        ).alias(\"doc_freq_diff\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Step 10: Find the most distinctive terms in each period\n",
    "# Terms that became more important after treatment\n",
    "more_important = vocab_table.filter(pl.col(\"tfidf_diff\") > 0).sort(\n",
    "    \"tfidf_diff\", descending=True\n",
    ")\n",
    "# Terms that became less important after treatment\n",
    "less_important = vocab_table.filter(pl.col(\"tfidf_diff\") < 0).sort(\n",
    "    \"tfidf_diff\", ascending=True\n",
    ")\n",
    "\n",
    "# Step 11: Find new terms that weren't significant before\n",
    "new_terms = (\n",
    "    vocab_table.filter(pl.col(\"pre_doc_freq\") < 0.01)\n",
    "    .filter(pl.col(\"post_doc_freq\") > 0.05)\n",
    "    .sort(\"post_doc_freq\", descending=True)\n",
    ")\n",
    "# Find terms that disappeared or significantly declined\n",
    "disappeared_terms = (\n",
    "    vocab_table.filter(pl.col(\"pre_doc_freq\") > 0.05)\n",
    "    .filter(pl.col(\"post_doc_freq\") < 0.01)\n",
    "    .sort(\"pre_doc_freq\", descending=True)\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(\"Top 20 terms with increased importance post-ChatGPT:\")\n",
    "print(more_important.head(20))\n",
    "\n",
    "print(\"\\nTop 20 terms with decreased importance post-ChatGPT:\")\n",
    "print(less_important.head(20))\n",
    "\n",
    "print(\"\\nTop 20 new or emerging terms post-ChatGPT:\")\n",
    "print(new_terms.head(20))\n",
    "\n",
    "print(\"\\nTop 20 disappearing terms post-ChatGPT:\")\n",
    "print(disappeared_terms.head(20))\n",
    "\n",
    "# Step 12: Visualize the results\n",
    "plt.figure(figsize=(12, 8))\n",
    "# Plot top 15 terms with biggest absolute change\n",
    "top_changed = (\n",
    "    vocab_table.filter(pl.col(\"pre_doc_freq\") > 0.01 | pl.col(\"post_doc_freq\") > 0.01)\n",
    "    .sort(\"tfidf_diff\", descending=True)\n",
    "    .head(15)\n",
    ")\n",
    "\n",
    "bottom_changed = (\n",
    "    vocab_table.filter(pl.col(\"pre_doc_freq\") > 0.01 | pl.col(\"post_doc_freq\") > 0.01)\n",
    "    .sort(\"tfidf_diff\", ascending=True)\n",
    "    .head(15)\n",
    ")\n",
    "\n",
    "# Combine for visualization\n",
    "to_plot = pl.concat([top_changed, bottom_changed])\n",
    "\n",
    "# Convert to pandas for Seaborn\n",
    "to_plot_pd = to_plot.to_pandas()\n",
    "\n",
    "# Create a barplot of the differences\n",
    "sns.barplot(data=to_plot_pd, x=\"tfidf_diff\", y=\"term\", palette=\"coolwarm\")\n",
    "plt.axvline(x=0, color=\"black\", linestyle=\"-\", alpha=0.3)\n",
    "plt.title(\"Terms with Biggest Change in TF-IDF Importance After ChatGPT\")\n",
    "plt.xlabel(\"Change in TF-IDF Score (Post - Pre)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../imgs/tfidf_change_plot.png\")\n",
    "plt.show()\n",
    "\n",
    "# Save the full vocabulary table\n",
    "vocab_table.write_parquet(\"../data/tfidf_vocabulary_comparison.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
