{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data extraction notebook\n",
    "\n",
    "This notebook extracts the posts data from the Stack .xml archive dumps and saves them to disk with an indicated period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import contextlib\n",
    "import gc\n",
    "import os\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import polars as pl\n",
    "import psutil\n",
    "import py7zr\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data from large archive files:\n",
    "https://archive.org/download/stackexchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_memory_usage(label=\"\"):\n",
    "    \"\"\"Log current memory usage\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_info = process.memory_info()\n",
    "    print(f\"Memory usage {label}: {mem_info.rss / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def temp_chunk_files(temp_dir, files_to_clean=None):\n",
    "    \"\"\"Context manager to handle temporary chunk files.\n",
    "\n",
    "    Creates a temporary directory if it doesn't exist and cleans up all files\n",
    "    in that directory when exiting the context.\n",
    "\n",
    "    Parameters:\n",
    "        temp_dir: Directory where temporary files are stored\n",
    "        files_to_clean: List of files to clean up (optional)\n",
    "    \"\"\"\n",
    "    # Create the temp directory if it doesn't exist\n",
    "    Path(temp_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Use provided list or create a new one\n",
    "    files_to_track = files_to_clean or []\n",
    "\n",
    "    try:\n",
    "        # Yield the list that will store paths to the temporary files\n",
    "        yield files_to_track\n",
    "    finally:\n",
    "        # Clean up all the temporary files when done\n",
    "        for file in files_to_track:\n",
    "            try:\n",
    "                os.remove(file)\n",
    "                print(f\"Removed temporary file: {file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Failed to remove temporary file {file}: {e}\")\n",
    "\n",
    "\n",
    "def process_xml_in_7z(\n",
    "    archive_path,\n",
    "    batch_size=5000,\n",
    "    start_date=None,\n",
    "    end_date=None,\n",
    "    record_tag=\"row\",\n",
    "    chunk_to_disk=False,\n",
    "    temp_dir=\"data/temp/\",\n",
    "    micro_batch_size=100,\n",
    "):\n",
    "    \"\"\"\n",
    "    Process an XML file within a 7z archive efficiently, optimized for Stack Exchange data.\n",
    "    Only filters by non-empty titles and date range.\n",
    "\n",
    "    Args:\n",
    "        archive_path (str): Path to the .7z archive\n",
    "        batch_size (int): Number of elements to process in each batch\n",
    "        start_date (str): Optional start date in format 'YYYY-MM-DD'\n",
    "        end_date (str): Optional end date in format 'YYYY-MM-DD'\n",
    "        record_tag (str): XML tag name for records to process (default: \"row\")\n",
    "        chunk_to_disk (bool): Whether to write intermediate chunks to disk (for very large files)\n",
    "        temp_dir (str): Directory to store temporary chunk files if chunking is enabled\n",
    "        micro_batch_size (int): Size of micro-batches for more frequent memory clearing\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: Polars DataFrame containing the processed data with all columns\n",
    "    \"\"\"\n",
    "    print(f\"Starting processing of {archive_path}\")\n",
    "    log_memory_usage(\"at start\")\n",
    "\n",
    "    # Convert date strings to datetime objects if provided\n",
    "    start_dt = datetime.strptime(start_date, \"%Y-%m-%d\") if start_date else None\n",
    "    end_dt = datetime.strptime(end_date, \"%Y-%m-%d\") if end_date else None\n",
    "\n",
    "    # Get the filename inside the archive\n",
    "    xml_filename = None\n",
    "    with py7zr.SevenZipFile(archive_path, mode=\"r\") as archive:\n",
    "        file_list = archive.getnames()\n",
    "        if not file_list:\n",
    "            raise ValueError(\"No files found in archive\")\n",
    "\n",
    "        # Look for Posts.xml\n",
    "        for filename in file_list:\n",
    "            if filename.endswith(\"Posts.xml\"):\n",
    "                xml_filename = filename\n",
    "                break\n",
    "\n",
    "        if not xml_filename:\n",
    "            # Just use the first file if Posts.xml isn't found\n",
    "            xml_filename = file_list[0]\n",
    "\n",
    "    print(f\"Processing XML file: {xml_filename}\")\n",
    "\n",
    "    # Use 7z command-line tool to pipe the content without extraction\n",
    "    cmd = [\"7z\", \"e\", \"-so\", archive_path, xml_filename]\n",
    "    print(f\"Executing: {' '.join(cmd)}\")\n",
    "\n",
    "    # Initialize tracking variables\n",
    "    all_data = []\n",
    "    micro_batch = []\n",
    "    total_processed = 0\n",
    "    total_skipped = 0\n",
    "    chunk_files = []\n",
    "    chunk_count = 0\n",
    "\n",
    "    # Create temp directory if chunking is enabled\n",
    "    if chunk_to_disk:\n",
    "        Path(temp_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Start the extraction process with controlled buffer size\n",
    "    process = subprocess.Popen(\n",
    "        cmd,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        bufsize=1024 * 1024,  # 1MB buffer\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Create iterator with lxml\n",
    "        context = etree.iterparse(\n",
    "            process.stdout,\n",
    "            events=(\"end\",),\n",
    "            tag=record_tag,\n",
    "            recover=True,\n",
    "            huge_tree=True,\n",
    "            remove_blank_text=True,\n",
    "            remove_comments=True,\n",
    "            remove_pis=True,\n",
    "        )\n",
    "\n",
    "        for _, elem in context:\n",
    "            # Only skip if it's not a question (PostTypeId=1)\n",
    "            post_type = elem.get(\"PostTypeId\")\n",
    "            if post_type != \"1\":\n",
    "                total_skipped += 1\n",
    "                elem.clear()\n",
    "                continue\n",
    "\n",
    "            # Date filter (if specified)\n",
    "            skip_record = False\n",
    "            if start_dt or end_dt:\n",
    "                date_attr = elem.get(\"CreationDate\")\n",
    "                if date_attr:\n",
    "                    try:\n",
    "                        # Parse date for filtering\n",
    "                        if \"T\" in date_attr:\n",
    "                            record_date = datetime.fromisoformat(\n",
    "                                date_attr.replace(\"Z\", \"+00:00\")\n",
    "                            )\n",
    "                        else:\n",
    "                            record_date = datetime.strptime(date_attr, \"%Y-%m-%d\")\n",
    "\n",
    "                        # Skip if out of date range\n",
    "                        if (start_dt and record_date.date() < start_dt.date()) or (\n",
    "                            end_dt and record_date.date() > end_dt.date()\n",
    "                        ):\n",
    "                            skip_record = True\n",
    "                    except (ValueError, TypeError) as e:\n",
    "                        print(f\"Warning: Invalid date format '{date_attr}', error: {e}\")\n",
    "\n",
    "            if skip_record:\n",
    "                total_skipped += 1\n",
    "                elem.clear()\n",
    "                continue\n",
    "\n",
    "            # If we reach here, the record should be included\n",
    "            # Extract ALL attributes - no filtering of columns\n",
    "            row_data = {\n",
    "                k: str(v) if v is not None else None for k, v in elem.attrib.items()\n",
    "            }\n",
    "\n",
    "            # Add to micro-batch\n",
    "            micro_batch.append(row_data)\n",
    "            total_processed += 1\n",
    "\n",
    "            # Clear element to free memory\n",
    "            elem.clear()\n",
    "            # Also eliminate previous siblings to keep memory usage low\n",
    "            while elem.getprevious() is not None:\n",
    "                del elem.getparent()[0]\n",
    "\n",
    "            # Process in micro-batches to avoid memory spikes\n",
    "            if len(micro_batch) >= micro_batch_size:\n",
    "                all_data.extend(micro_batch)\n",
    "                micro_batch = []  # Free the micro-batch memory\n",
    "\n",
    "                # If we've reached full batch size, process the batch\n",
    "                if len(all_data) >= batch_size:\n",
    "                    if chunk_to_disk:\n",
    "                        # Create and save dataframe chunk\n",
    "                        chunk_df = pl.from_dicts(\n",
    "                            all_data, infer_schema_length=min(100000, len(all_data))\n",
    "                        )\n",
    "                        chunk_file = f\"{temp_dir}/chunk_{chunk_count}.parquet\"\n",
    "\n",
    "                        # Write to parquet with compression\n",
    "                        chunk_df.write_parquet(chunk_file, compression=\"zstd\")\n",
    "                        chunk_files.append(chunk_file)\n",
    "                        chunk_count += 1\n",
    "\n",
    "                        # Clear memory\n",
    "                        del chunk_df\n",
    "                        all_data = []  # Free memory\n",
    "                        gc.collect()\n",
    "\n",
    "                    if total_processed % 50000 == 0:\n",
    "                        print(\n",
    "                            f\"Processed {total_processed:,} records, skipped {total_skipped:,}\"\n",
    "                        )\n",
    "                        log_memory_usage(\"during processing\")\n",
    "\n",
    "        # Process any remaining data in the micro-batch\n",
    "        if micro_batch:\n",
    "            all_data.extend(micro_batch)\n",
    "            micro_batch = []  # Free memory\n",
    "\n",
    "        # Process final batch if there's any data left\n",
    "        if all_data and chunk_to_disk:\n",
    "            chunk_df = pl.from_dicts(\n",
    "                all_data, infer_schema_length=min(100000, len(all_data))\n",
    "            )\n",
    "            chunk_file = f\"{temp_dir}/chunk_{chunk_count}.parquet\"\n",
    "            chunk_df.write_parquet(chunk_file, compression=\"zstd\")\n",
    "            chunk_files.append(chunk_file)\n",
    "            del chunk_df\n",
    "            all_data = []  # Free memory\n",
    "            gc.collect()\n",
    "\n",
    "        print(\"All records processed. Creating final DataFrame...\")\n",
    "\n",
    "    finally:\n",
    "        # Terminate the subprocess if it's still running\n",
    "        if process.poll() is None:\n",
    "            process.terminate()\n",
    "            process.wait(timeout=5)  # Wait for process to terminate\n",
    "\n",
    "    # Final results processing\n",
    "    print(\n",
    "        f\"Completed. Total processed: {total_processed:,}, skipped: {total_skipped:,}\"\n",
    "    )\n",
    "\n",
    "    if chunk_to_disk and chunk_files:\n",
    "        # Save the list of files we want to process\n",
    "        files_to_process = chunk_files.copy()\n",
    "\n",
    "        with temp_chunk_files(temp_dir, files_to_process) as _:\n",
    "            print(f\"Reading and combining {len(files_to_process)} chunks from disk\")\n",
    "\n",
    "            # For very large datasets, combine chunks incrementally to prevent memory overload\n",
    "            if len(files_to_process) > 0:\n",
    "                # Start with the first chunk\n",
    "                result_df = pl.read_parquet(files_to_process[0])\n",
    "\n",
    "                # Process chunks in groups to limit memory usage\n",
    "                for i, file in enumerate(files_to_process[1:], 1):\n",
    "                    if i % 10 == 0:\n",
    "                        print(f\"Combining chunk {i}/{len(files_to_process)}\")\n",
    "                        log_memory_usage(f\"after {i} chunks\")\n",
    "\n",
    "                    # Read the chunk\n",
    "                    try:\n",
    "                        next_df = pl.read_parquet(file)\n",
    "\n",
    "                        # Combine with result\n",
    "                        result_df = pl.concat(\n",
    "                            [result_df, next_df], how=\"diagonal_relaxed\"\n",
    "                        )\n",
    "\n",
    "                        # Release memory\n",
    "                        del next_df\n",
    "                        if i % 5 == 0:  # Periodically collect garbage\n",
    "                            gc.collect()\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading chunk {file}: {e}\")\n",
    "                        continue\n",
    "\n",
    "                print(f\"Final dataframe size: {len(result_df):,} rows\")\n",
    "                log_memory_usage(\"after combining all chunks\")\n",
    "                return result_df\n",
    "            else:\n",
    "                print(\"No chunks were created. Returning empty DataFrame.\")\n",
    "                return pl.DataFrame()\n",
    "\n",
    "    else:\n",
    "        # Process in-memory data\n",
    "        if all_data:\n",
    "            print(f\"Creating DataFrame from {len(all_data):,} records...\")\n",
    "            df = pl.from_dicts(all_data, infer_schema_length=min(100000, len(all_data)))\n",
    "            # Clear the all_data list to free memory\n",
    "            all_data = []\n",
    "            gc.collect()\n",
    "            return df\n",
    "        else:\n",
    "            return pl.DataFrame()\n",
    "\n",
    "\n",
    "def process_stack_data(\n",
    "    archive_path,\n",
    "    output_file=\"stack_data.parquet\",\n",
    "    start_date=None,\n",
    "    end_date=None,\n",
    "    batch_size=5000,\n",
    "    large_file=False,\n",
    "    split_output=False,\n",
    "    max_rows_per_file=1000000,\n",
    "    temp_dir=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    High-level function to process Stack Exchange XML data from any community.\n",
    "    Keeps all columns and only filters out empty titles (for questions) and by date.\n",
    "\n",
    "    Args:\n",
    "        archive_path (str): Path to the 7z archive containing XML data\n",
    "        output_file (str): Path to save the output parquet file\n",
    "        start_date (str): Optional start date filter in 'YYYY-MM-DD' format\n",
    "        end_date (str): Optional end date filter in 'YYYY-MM-DD' format\n",
    "        batch_size (int): Batch size for processing\n",
    "        large_file (bool): If True, use disk-based chunking for very large files\n",
    "        split_output (bool): If True, split output into multiple files for memory efficiency\n",
    "        max_rows_per_file (int): Maximum rows per file when splitting output\n",
    "        temp_dir (str): Directory to store temporary chunk files\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame or None: Processed data, or None if split_output is True\n",
    "    \"\"\"\n",
    "    print(f\"Processing {archive_path}\")\n",
    "    log_memory_usage(\"before processing\")\n",
    "\n",
    "    # Set a community-specific temp directory if not provided\n",
    "    if temp_dir is None:\n",
    "        community_name = os.path.basename(archive_path).split(\".\")[0]\n",
    "        temp_dir = f\"data/temp/{community_name}/\"\n",
    "\n",
    "    # Always use chunk_to_disk for large_file processing\n",
    "    df = process_xml_in_7z(\n",
    "        archive_path=archive_path,\n",
    "        batch_size=batch_size,\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "        chunk_to_disk=large_file,\n",
    "        temp_dir=temp_dir,\n",
    "    )\n",
    "\n",
    "    if not df.is_empty():\n",
    "        print(f\"Found {len(df):,} records\")\n",
    "        print(f\"Columns: {df.columns}\")\n",
    "\n",
    "        # For very large result sets, split the output into multiple files\n",
    "        if split_output and len(df) > max_rows_per_file:\n",
    "            output_base, output_ext = output_file.rsplit(\".\", 1)\n",
    "            num_files = (len(df) + max_rows_per_file - 1) // max_rows_per_file\n",
    "\n",
    "            print(\n",
    "                f\"Splitting output into {num_files} files with max {max_rows_per_file:,} rows each\"\n",
    "            )\n",
    "\n",
    "            for i in range(num_files):\n",
    "                start_idx = i * max_rows_per_file\n",
    "                end_idx = min((i + 1) * max_rows_per_file, len(df))\n",
    "\n",
    "                # Get slice of dataframe\n",
    "                part_df = df.slice(start_idx, end_idx - start_idx)\n",
    "\n",
    "                # Save to file\n",
    "                part_file = f\"{output_base}_part{i + 1}.{output_ext}\"\n",
    "                part_df.write_parquet(part_file, compression=\"zstd\")\n",
    "                print(f\"Saved part {i + 1}/{num_files} to {part_file}\")\n",
    "\n",
    "                # Release memory\n",
    "                del part_df\n",
    "                gc.collect()\n",
    "\n",
    "            # Free the main dataframe memory\n",
    "            del df\n",
    "            gc.collect()\n",
    "            log_memory_usage(\"after saving split files\")\n",
    "            return None  # Return None since we've split the output\n",
    "        else:\n",
    "            # Save to single parquet file\n",
    "            print(f\"Saving data to {output_file}\")\n",
    "            df.write_parquet(output_file, compression=\"zstd\")\n",
    "            print(f\"Data saved to {output_file}\")\n",
    "            log_memory_usage(\"after saving\")\n",
    "            return df\n",
    "    else:\n",
    "        print(\"No matching records found\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data/stackoverflow/stackoverflow.com-Posts.7z\n",
      "Memory usage before processing: 285.81 MB\n",
      "Starting processing of data/stackoverflow/stackoverflow.com-Posts.7z\n",
      "Memory usage at start: 285.81 MB\n",
      "Processing XML file: Posts.xml\n",
      "Executing: 7z e -so data/stackoverflow/stackoverflow.com-Posts.7z Posts.xml\n",
      "Processed 100,000 records, skipped 50,941,121\n",
      "Memory usage during processing: 7533.20 MB\n",
      "Processed 200,000 records, skipped 51,071,796\n",
      "Memory usage during processing: 7571.72 MB\n",
      "Processed 300,000 records, skipped 51,200,118\n",
      "Memory usage during processing: 7594.03 MB\n",
      "Processed 400,000 records, skipped 51,327,532\n",
      "Memory usage during processing: 7614.73 MB\n",
      "Processed 500,000 records, skipped 51,455,391\n",
      "Memory usage during processing: 7634.89 MB\n",
      "Processed 600,000 records, skipped 51,582,399\n",
      "Memory usage during processing: 7654.01 MB\n",
      "Processed 700,000 records, skipped 51,712,021\n",
      "Memory usage during processing: 7664.99 MB\n",
      "Processed 800,000 records, skipped 51,841,745\n",
      "Memory usage during processing: 7670.76 MB\n",
      "Processed 900,000 records, skipped 51,970,207\n",
      "Memory usage during processing: 7677.29 MB\n",
      "Processed 1,000,000 records, skipped 52,100,884\n",
      "Memory usage during processing: 7686.09 MB\n",
      "Processed 1,100,000 records, skipped 52,233,249\n",
      "Memory usage during processing: 7688.21 MB\n",
      "Processed 1,200,000 records, skipped 52,363,961\n",
      "Memory usage during processing: 7691.28 MB\n",
      "Processed 1,300,000 records, skipped 52,493,852\n",
      "Memory usage during processing: 7698.12 MB\n",
      "Processed 1,400,000 records, skipped 52,620,336\n",
      "Memory usage during processing: 7703.02 MB\n",
      "Processed 1,500,000 records, skipped 52,747,369\n",
      "Memory usage during processing: 7701.61 MB\n",
      "Processed 1,600,000 records, skipped 52,878,262\n",
      "Memory usage during processing: 7706.49 MB\n",
      "Processed 1,700,000 records, skipped 53,006,629\n",
      "Memory usage during processing: 7711.86 MB\n",
      "Processed 1,800,000 records, skipped 53,132,055\n",
      "Memory usage during processing: 7713.23 MB\n",
      "Processed 1,900,000 records, skipped 53,256,502\n",
      "Memory usage during processing: 7726.44 MB\n",
      "Processed 2,000,000 records, skipped 53,382,436\n",
      "Memory usage during processing: 7725.52 MB\n",
      "Processed 2,100,000 records, skipped 53,508,894\n",
      "Memory usage during processing: 7731.78 MB\n",
      "Processed 2,200,000 records, skipped 53,636,291\n",
      "Memory usage during processing: 7733.62 MB\n",
      "Processed 2,300,000 records, skipped 53,765,108\n",
      "Memory usage during processing: 7734.34 MB\n",
      "Processed 2,400,000 records, skipped 53,893,486\n",
      "Memory usage during processing: 7745.44 MB\n",
      "Processed 2,500,000 records, skipped 54,026,372\n",
      "Memory usage during processing: 7743.68 MB\n",
      "Processed 2,600,000 records, skipped 54,158,262\n",
      "Memory usage during processing: 7746.59 MB\n",
      "Processed 2,700,000 records, skipped 54,286,997\n",
      "Memory usage during processing: 7746.88 MB\n",
      "Processed 2,800,000 records, skipped 54,413,353\n",
      "Memory usage during processing: 7760.07 MB\n",
      "Processed 2,900,000 records, skipped 54,541,160\n",
      "Memory usage during processing: 7767.75 MB\n",
      "Processed 3,000,000 records, skipped 54,670,635\n",
      "Memory usage during processing: 7770.59 MB\n",
      "Processed 3,100,000 records, skipped 54,799,096\n",
      "Memory usage during processing: 7777.18 MB\n",
      "Processed 3,200,000 records, skipped 54,923,193\n",
      "Memory usage during processing: 7793.23 MB\n",
      "Processed 3,300,000 records, skipped 55,023,354\n",
      "Memory usage during processing: 7800.77 MB\n",
      "Processed 3,400,000 records, skipped 55,121,655\n",
      "Memory usage during processing: 7803.99 MB\n",
      "Processed 3,500,000 records, skipped 55,221,578\n",
      "Memory usage during processing: 7808.78 MB\n",
      "Processed 3,600,000 records, skipped 55,324,427\n",
      "Memory usage during processing: 7812.97 MB\n",
      "Processed 3,700,000 records, skipped 55,427,536\n",
      "Memory usage during processing: 7813.98 MB\n",
      "Processed 3,800,000 records, skipped 55,530,651\n",
      "Memory usage during processing: 7815.91 MB\n",
      "Processed 3,900,000 records, skipped 55,635,402\n",
      "Memory usage during processing: 7825.13 MB\n",
      "Processed 4,000,000 records, skipped 55,737,149\n",
      "Memory usage during processing: 7832.95 MB\n",
      "All records processed. Creating final DataFrame...\n",
      "Completed. Total processed: 4,043,144, skipped: 55,775,904\n",
      "Reading and combining 41 chunks from disk\n",
      "Combining chunk 10/41\n",
      "Memory usage after 10 chunks: 9996.95 MB\n",
      "Combining chunk 20/41\n",
      "Memory usage after 20 chunks: 11849.68 MB\n",
      "Combining chunk 30/41\n",
      "Memory usage after 30 chunks: 14110.81 MB\n",
      "Combining chunk 40/41\n",
      "Memory usage after 40 chunks: 16591.32 MB\n",
      "Final dataframe size: 4,043,144 rows\n",
      "Memory usage after combining all chunks: 16692.79 MB\n",
      "Removed temporary file: data/temp/stackoverflow//chunk_0.parquet\n",
      "Removed temporary file: data/temp/stackoverflow//chunk_1.parquet\n",
      "Removed temporary file: data/temp/stackoverflow//chunk_2.parquet\n",
      "Removed temporary file: data/temp/stackoverflow//chunk_3.parquet\n",
      "Removed temporary file: data/temp/stackoverflow//chunk_4.parquet\n",
      "Removed temporary file: data/temp/stackoverflow//chunk_5.parquet\n",
      "Removed temporary file: data/temp/stackoverflow//chunk_6.parquet\n",
      "Removed temporary file: data/temp/stackoverflow//chunk_7.parquet\n",
      "Removed temporary file: data/temp/stackoverflow//chunk_8.parquet\n",
      "Removed temporary file: data/temp/stackoverflow//chunk_9.parquet\n",
      "Removed temporary file: data/temp/stackoverflow//chunk_10.parquet\n",
      "Removed temporary file: data/temp/stackoverflow//chunk_11.parquet\n",
      "Removed temporary file: data/temp/stackoverflow//chunk_12.parquet\n",
      "Removed temporary file: data/temp/stackoverflow//chunk_13.parquet\n",
      "Removed temporary file: data/temp/stackoverflow//chunk_14.parquet\n",
      "Removed temporary file: data/temp/stackoverflow//chunk_15.parquet\n",
      "Removed temporary file: data/temp/stackoverflow//chunk_16.parquet\n",
      "Removed temporary file: data/temp/stackoverflow//chunk_17.parquet\n",
      "Removed temporary file: data/temp/stackoverflow//chunk_18.parquet\n",
      "Removed temporary file: data/temp/stackoverflow//chunk_19.parquet\n",
      "Removed temporary file: data/temp/stackoverflow//chunk_20.parquet\n",
      "Removed temporary file: data/temp/stackoverflow//chunk_21.parquet\n",
      "Removed temporary file: data/temp/stackoverflow//chunk_22.parquet\n",
      "Removed temporary file: data/temp/stackoverflow//chunk_23.parquet\n",
      "Removed temporary file: data/temp/stackoverflow//chunk_24.parquet\n",
      "Removed temporary file: data/temp/stackoverflow//chunk_25.parquet\n",
      "Removed temporary file: data/temp/stackoverflow//chunk_26.parquet\n",
      "Removed temporary file: data/temp/stackoverflow//chunk_27.parquet\n",
      "Removed temporary file: data/temp/stackoverflow//chunk_28.parquet\n",
      "Removed temporary file: data/temp/stackoverflow//chunk_29.parquet\n",
      "Removed temporary file: data/temp/stackoverflow//chunk_30.parquet\n",
      "Removed temporary file: data/temp/stackoverflow//chunk_31.parquet\n",
      "Removed temporary file: data/temp/stackoverflow//chunk_32.parquet\n",
      "Removed temporary file: data/temp/stackoverflow//chunk_33.parquet\n",
      "Removed temporary file: data/temp/stackoverflow//chunk_34.parquet\n",
      "Removed temporary file: data/temp/stackoverflow//chunk_35.parquet\n",
      "Removed temporary file: data/temp/stackoverflow//chunk_36.parquet\n",
      "Removed temporary file: data/temp/stackoverflow//chunk_37.parquet\n",
      "Removed temporary file: data/temp/stackoverflow//chunk_38.parquet\n",
      "Removed temporary file: data/temp/stackoverflow//chunk_39.parquet\n",
      "Removed temporary file: data/temp/stackoverflow//chunk_40.parquet\n",
      "Found 4,043,144 records\n",
      "Columns: ['Id', 'PostTypeId', 'AcceptedAnswerId', 'CreationDate', 'Score', 'ViewCount', 'Body', 'OwnerUserId', 'LastEditorUserId', 'LastEditDate', 'LastActivityDate', 'Title', 'Tags', 'AnswerCount', 'CommentCount', 'ContentLicense', 'FavoriteCount', 'ClosedDate', 'OwnerDisplayName', 'LastEditorDisplayName', 'CommunityOwnedDate']\n",
      "Saving data to data/stackoverflow/stackoverflow.parquet\n",
      "Data saved to data/stackoverflow/stackoverflow.parquet\n",
      "Memory usage after saving: 18229.04 MB\n"
     ]
    }
   ],
   "source": [
    "# Extract StackOverflow data from xml in 7z archive\n",
    "df = process_stack_data(\n",
    "    \"../data/stackoverflow/stackoverflow.com-Posts.7z\",\n",
    "    output_file=\"../data/stackoverflow/stackoverflow.parquet\",\n",
    "    batch_size=100000,\n",
    "    start_date=\"2021-01-01\",\n",
    "    end_date=\"2024-12-31\",\n",
    "    large_file=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data/law/law.stackexchange.com.7z\n",
      "Memory usage before processing: 281.58 MB\n",
      "Starting processing of data/law/law.stackexchange.com.7z\n",
      "Memory usage at start: 281.58 MB\n",
      "Processing XML file: Posts.xml\n",
      "Executing: 7z e -so data/law/law.stackexchange.com.7z Posts.xml\n",
      "All records processed. Creating final DataFrame...\n",
      "Completed. Total processed: 11,325, skipped: 62,019\n",
      "Reading and combining 1 chunks from disk\n",
      "Final dataframe size: 11,325 rows\n",
      "Memory usage after combining all chunks: 444.19 MB\n",
      "Removed temporary file: data/temp/law//chunk_0.parquet\n",
      "Found 11,325 records\n",
      "Columns: ['Id', 'PostTypeId', 'CreationDate', 'Score', 'ViewCount', 'Body', 'OwnerUserId', 'LastEditorUserId', 'LastEditDate', 'LastActivityDate', 'Title', 'Tags', 'AnswerCount', 'CommentCount', 'ClosedDate', 'ContentLicense', 'LastEditorDisplayName', 'AcceptedAnswerId', 'OwnerDisplayName', 'FavoriteCount']\n",
      "Saving data to data/law/law.parquet\n",
      "Data saved to data/law/law.parquet\n",
      "Memory usage after saving: 447.36 MB\n"
     ]
    }
   ],
   "source": [
    "df_law = process_stack_data(\n",
    "    \"../data/law/law.stackexchange.com.7z\",\n",
    "    output_file=\"../data/law/law.parquet\",\n",
    "    batch_size=100000,\n",
    "    start_date=\"2021-01-01\",\n",
    "    end_date=\"2024-12-31\",\n",
    "    large_file=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data/academia/academia.stackexchange.com.7z\n",
      "Memory usage before processing: 447.36 MB\n",
      "Starting processing of data/academia/academia.stackexchange.com.7z\n",
      "Memory usage at start: 447.36 MB\n",
      "Processing XML file: Posts.xml\n",
      "Executing: 7z e -so data/academia/academia.stackexchange.com.7z Posts.xml\n",
      "All records processed. Creating final DataFrame...\n",
      "Completed. Total processed: 9,992, skipped: 138,814\n",
      "Reading and combining 1 chunks from disk\n",
      "Final dataframe size: 9,992 rows\n",
      "Memory usage after combining all chunks: 471.69 MB\n",
      "Removed temporary file: data/temp/academia//chunk_0.parquet\n",
      "Found 9,992 records\n",
      "Columns: ['Id', 'PostTypeId', 'CreationDate', 'Score', 'ViewCount', 'Body', 'OwnerUserId', 'LastActivityDate', 'Title', 'Tags', 'AnswerCount', 'CommentCount', 'ContentLicense', 'AcceptedAnswerId', 'LastEditorUserId', 'LastEditDate', 'ClosedDate', 'FavoriteCount', 'OwnerDisplayName', 'LastEditorDisplayName', 'CommunityOwnedDate']\n",
      "Saving data to data/academia/academia.parquet\n",
      "Data saved to data/academia/academia.parquet\n",
      "Memory usage after saving: 474.72 MB\n"
     ]
    }
   ],
   "source": [
    "df_ac = process_stack_data(\n",
    "    \"../data/academia/academia.stackexchange.com.7z\",\n",
    "    output_file=\"../data/academia/academia.parquet\",\n",
    "    batch_size=100000,\n",
    "    start_date=\"2021-01-01\",\n",
    "    end_date=\"2024-12-31\",\n",
    "    large_file=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data/physics/physics.stackexchange.com.7z\n",
      "Memory usage before processing: 474.72 MB\n",
      "Starting processing of data/physics/physics.stackexchange.com.7z\n",
      "Memory usage at start: 474.72 MB\n",
      "Processing XML file: Posts.xml\n",
      "Executing: 7z e -so data/physics/physics.stackexchange.com.7z Posts.xml\n",
      "All records processed. Creating final DataFrame...\n",
      "Completed. Total processed: 62,307, skipped: 514,994\n",
      "Reading and combining 1 chunks from disk\n",
      "Final dataframe size: 62,307 rows\n",
      "Memory usage after combining all chunks: 841.76 MB\n",
      "Removed temporary file: data/temp/physics//chunk_0.parquet\n",
      "Found 62,307 records\n",
      "Columns: ['Id', 'PostTypeId', 'AcceptedAnswerId', 'CreationDate', 'Score', 'ViewCount', 'Body', 'OwnerUserId', 'LastActivityDate', 'Title', 'Tags', 'AnswerCount', 'CommentCount', 'ContentLicense', 'LastEditorUserId', 'LastEditDate', 'ClosedDate', 'CommunityOwnedDate', 'OwnerDisplayName', 'FavoriteCount', 'LastEditorDisplayName']\n",
      "Saving data to data/physics/physics.parquet\n",
      "Data saved to data/physics/physics.parquet\n",
      "Memory usage after saving: 860.75 MB\n"
     ]
    }
   ],
   "source": [
    "df_ph = process_stack_data(\n",
    "    \"../data/physics/physics.stackexchange.com.7z\",\n",
    "    output_file=\"../data/physics/physics.parquet\",\n",
    "    batch_size=100000,\n",
    "    start_date=\"2021-01-01\",\n",
    "    end_date=\"2024-12-31\",\n",
    "    large_file=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data/superuser/superuser.com.7z\n",
      "Memory usage before processing: 861.01 MB\n",
      "Starting processing of data/superuser/superuser.com.7z\n",
      "Memory usage at start: 861.01 MB\n",
      "Processing XML file: Posts.xml\n",
      "Executing: 7z e -so data/superuser/superuser.com.7z Posts.xml\n",
      "All records processed. Creating final DataFrame...\n",
      "Completed. Total processed: 65,775, skipped: 1,179,643\n",
      "Reading and combining 1 chunks from disk\n",
      "Final dataframe size: 65,775 rows\n",
      "Memory usage after combining all chunks: 1074.66 MB\n",
      "Removed temporary file: data/temp/superuser//chunk_0.parquet\n",
      "Found 65,775 records\n",
      "Columns: ['Id', 'PostTypeId', 'CreationDate', 'Score', 'ViewCount', 'Body', 'OwnerUserId', 'LastActivityDate', 'Title', 'Tags', 'AnswerCount', 'CommentCount', 'ContentLicense', 'LastEditorUserId', 'LastEditDate', 'AcceptedAnswerId', 'ClosedDate', 'FavoriteCount', 'OwnerDisplayName', 'LastEditorDisplayName']\n",
      "Saving data to data/superuser/superuser.parquet\n",
      "Data saved to data/superuser/superuser.parquet\n",
      "Memory usage after saving: 1082.36 MB\n"
     ]
    }
   ],
   "source": [
    "df_su = process_stack_data(\n",
    "    \"../data/superuser/superuser.com.7z\",\n",
    "    output_file=\"../data/superuser/superuser.parquet\",\n",
    "    batch_size=100000,\n",
    "    start_date=\"2021-01-01\",\n",
    "    end_date=\"2024-12-31\",\n",
    "    large_file=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data/askubuntu/askubuntu.com.7z\n",
      "Memory usage before processing: 1082.36 MB\n",
      "Starting processing of data/askubuntu/askubuntu.com.7z\n",
      "Memory usage at start: 1082.36 MB\n",
      "Processing XML file: Posts.xml\n",
      "Executing: 7z e -so data/askubuntu/askubuntu.com.7z Posts.xml\n",
      "All records processed. Creating final DataFrame...\n",
      "Completed. Total processed: 64,791, skipped: 880,320\n",
      "Reading and combining 1 chunks from disk\n",
      "Final dataframe size: 64,791 rows\n",
      "Memory usage after combining all chunks: 1245.04 MB\n",
      "Removed temporary file: data/temp/askubuntu//chunk_0.parquet\n",
      "Found 64,791 records\n",
      "Columns: ['Id', 'PostTypeId', 'CreationDate', 'Score', 'ViewCount', 'Body', 'OwnerUserId', 'LastActivityDate', 'Title', 'Tags', 'AnswerCount', 'CommentCount', 'ClosedDate', 'ContentLicense', 'FavoriteCount', 'OwnerDisplayName', 'LastEditorDisplayName', 'LastEditDate', 'AcceptedAnswerId', 'LastEditorUserId', 'CommunityOwnedDate']\n",
      "Saving data to data/askubuntu/askubuntu.parquet\n",
      "Data saved to data/askubuntu/askubuntu.parquet\n",
      "Memory usage after saving: 1257.96 MB\n"
     ]
    }
   ],
   "source": [
    "df_au = process_stack_data(\n",
    "    \"../data/askubuntu/askubuntu.com.7z\",\n",
    "    output_file=\"../data/askubuntu/askubuntu.parquet\",\n",
    "    batch_size=100000,\n",
    "    start_date=\"2021-01-01\",\n",
    "    end_date=\"2024-12-31\",\n",
    "    large_file=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data/math/math.stackexchange.com.7z\n",
      "Memory usage before processing: 1257.96 MB\n",
      "Starting processing of data/math/math.stackexchange.com.7z\n",
      "Memory usage at start: 1257.96 MB\n",
      "Processing XML file: Posts.xml\n",
      "Executing: 7z e -so data/math/math.stackexchange.com.7z Posts.xml\n",
      "Processed 100,000 records, skipped 3,253,522\n",
      "Memory usage during processing: 1663.37 MB\n",
      "Processed 200,000 records, skipped 3,354,510\n",
      "Memory usage during processing: 1683.61 MB\n",
      "Processed 300,000 records, skipped 3,450,219\n",
      "Memory usage during processing: 1701.33 MB\n",
      "All records processed. Creating final DataFrame...\n",
      "Completed. Total processed: 321,580, skipped: 3,470,855\n",
      "Reading and combining 4 chunks from disk\n",
      "Final dataframe size: 321,580 rows\n",
      "Memory usage after combining all chunks: 2337.84 MB\n",
      "Removed temporary file: data/temp/math//chunk_0.parquet\n",
      "Removed temporary file: data/temp/math//chunk_1.parquet\n",
      "Removed temporary file: data/temp/math//chunk_2.parquet\n",
      "Removed temporary file: data/temp/math//chunk_3.parquet\n",
      "Found 321,580 records\n",
      "Columns: ['Id', 'PostTypeId', 'AcceptedAnswerId', 'CreationDate', 'Score', 'ViewCount', 'Body', 'OwnerUserId', 'LastActivityDate', 'Title', 'Tags', 'AnswerCount', 'CommentCount', 'ContentLicense', 'LastEditorUserId', 'LastEditDate', 'ClosedDate', 'OwnerDisplayName', 'LastEditorDisplayName', 'FavoriteCount', 'CommunityOwnedDate']\n",
      "Saving data to data/math/math.parquet\n",
      "Data saved to data/math/math.parquet\n",
      "Memory usage after saving: 2533.47 MB\n"
     ]
    }
   ],
   "source": [
    "df_math = process_stack_data(\n",
    "    archive_path=\"../data/math/math.stackexchange.com.7z\",\n",
    "    output_file=\"../data/math/math.parquet\",\n",
    "    batch_size=100000,\n",
    "    start_date=\"2021-01-01\",\n",
    "    end_date=\"2024-12-31\",\n",
    "    large_file=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
