{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from datetime import datetime\n",
    "\n",
    "import lxml.etree as ET\n",
    "import polars as pl\n",
    "import py7zr\n",
    "\n",
    "\n",
    "def process_xml_in_7z_with_7z_executable(\n",
    "    archive_path,\n",
    "    batch_size=1000,\n",
    "    date_xpath=\".//CreationDate\",\n",
    "    start_date=None,\n",
    "    end_date=None,\n",
    "    record_tag=\"row\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Process an XML file within a 7z archive using the 7z command line tool to pipe data.\n",
    "    This approach avoids extracting to disk.\n",
    "\n",
    "    Args:\n",
    "        archive_path (str): Path to the .7z archive\n",
    "        batch_size (int): Number of elements to process in each batch\n",
    "        date_xpath (str): XPath to the date element within each record\n",
    "        start_date (str): Optional start date in format 'YYYY-MM-DD'\n",
    "        end_date (str): Optional end date in format 'YYYY-MM-DD'\n",
    "        record_tag (str): XML tag name for records to process\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: Polars DataFrame containing the processed data\n",
    "    \"\"\"\n",
    "    # Convert date strings to datetime objects if provided\n",
    "    start_dt = datetime.strptime(start_date, \"%Y-%m-%d\") if start_date else None\n",
    "    end_dt = datetime.strptime(end_date, \"%Y-%m-%d\") if end_date else None\n",
    "\n",
    "    # Get the filename inside the archive\n",
    "    with py7zr.SevenZipFile(archive_path, mode=\"r\") as archive:\n",
    "        file_list = archive.getnames()\n",
    "        if not file_list:\n",
    "            raise ValueError(\"No files found in archive\")\n",
    "        xml_filename = file_list[0]\n",
    "\n",
    "    # Use 7z command-line tool to pipe the content without extraction\n",
    "    cmd = [\"7z\", \"e\", \"-so\", archive_path, xml_filename]\n",
    "    print(f\"Executing: {' '.join(cmd)}\")\n",
    "\n",
    "    # Initialize data lists\n",
    "    all_data = []\n",
    "    batch = []\n",
    "    total_processed = 0\n",
    "    total_skipped = 0\n",
    "\n",
    "    # Create a subprocess to stream the data\n",
    "    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "    # Use lxml's iterparse to process the stream\n",
    "    context = ET.iterparse(\n",
    "        process.stdout, events=(\"end\",), tag=record_tag, recover=True, huge_tree=True\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        for event, elem in context:\n",
    "            # Extract date value from attributes (common in StackOverflow XML)\n",
    "            date_attr = elem.get(\"CreationDate\")\n",
    "\n",
    "            if date_attr:\n",
    "                date_str = date_attr\n",
    "                try:\n",
    "                    # Handle ISO format dates (common in StackOverflow)\n",
    "                    if \"T\" in date_str:\n",
    "                        record_date = datetime.fromisoformat(\n",
    "                            date_str.replace(\"Z\", \"+00:00\")\n",
    "                        )\n",
    "                    else:\n",
    "                        record_date = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "\n",
    "                    # Apply date filtering\n",
    "                    date_in_range = True\n",
    "                    if start_dt and record_date.date() < start_dt.date():\n",
    "                        date_in_range = False\n",
    "                    if end_dt and record_date.date() > end_dt.date():\n",
    "                        date_in_range = False\n",
    "\n",
    "                    if date_in_range:\n",
    "                        # Extract all attributes into a dictionary\n",
    "                        row_data = dict(elem.attrib)\n",
    "                        batch.append(row_data)\n",
    "                    else:\n",
    "                        total_skipped += 1\n",
    "                except (ValueError, TypeError) as e:\n",
    "                    print(\n",
    "                        f\"Skipping record with invalid date format: {date_str}, error: {e}\"\n",
    "                    )\n",
    "                    total_skipped += 1\n",
    "\n",
    "            # Process in batches\n",
    "            if len(batch) >= batch_size:\n",
    "                all_data.extend(batch)\n",
    "                total_processed += len(batch)\n",
    "                print(\n",
    "                    f\"Processed {total_processed} records, skipped {total_skipped} records\"\n",
    "                )\n",
    "                batch = []\n",
    "\n",
    "            # Clear the element from memory\n",
    "            elem.clear()\n",
    "            # Also eliminate now-empty references from the root node to elem\n",
    "            while elem.getprevious() is not None:\n",
    "                del elem.getparent()[0]\n",
    "\n",
    "            # Every 10,000 records, convert current data to DataFrame to free memory\n",
    "            if total_processed > 0 and total_processed % 10000 == 0:\n",
    "                if all_data:\n",
    "                    # Convert to polars dataframe\n",
    "                    temp_df = pl.from_dicts(all_data)\n",
    "                    if \"df\" not in locals():\n",
    "                        df = temp_df\n",
    "                    else:\n",
    "                        # Append to existing dataframe\n",
    "                        df = pl.concat([df, temp_df])\n",
    "                    # Clear the all_data list to free memory\n",
    "                    all_data = []\n",
    "\n",
    "    finally:\n",
    "        # Terminate the subprocess if it's still running\n",
    "        if process.poll() is None:\n",
    "            process.terminate()\n",
    "\n",
    "    # Process any remaining items\n",
    "    if batch:\n",
    "        all_data.extend(batch)\n",
    "        total_processed += len(batch)\n",
    "\n",
    "    print(f\"Completed. Total processed: {total_processed}, skipped: {total_skipped}\")\n",
    "\n",
    "    # Convert remaining data to DataFrame\n",
    "    if all_data:\n",
    "        temp_df = pl.from_dicts(all_data)\n",
    "        if \"df\" in locals():\n",
    "            df = pl.concat([df, temp_df])\n",
    "        else:\n",
    "            df = temp_df\n",
    "\n",
    "    if \"df\" in locals() and not df.is_empty():\n",
    "        print(\n",
    "            f\"Created Polars DataFrame with {len(df)} rows and {len(df.columns)} columns\"\n",
    "        )\n",
    "        return df\n",
    "    else:\n",
    "        print(\"No data matched the criteria\")\n",
    "        return pl.DataFrame()\n",
    "\n",
    "\n",
    "def chunked_process_xml_in_7z(\n",
    "    archive_path,\n",
    "    batch_size=1000,\n",
    "    date_xpath=\".//CreationDate\",\n",
    "    start_date=None,\n",
    "    end_date=None,\n",
    "    record_tag=\"row\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Process an XML file within a 7z archive in chunks, writing intermediate results to parquet.\n",
    "    This approach uses very little memory.\n",
    "    \"\"\"\n",
    "    # Convert date strings to datetime objects if provided\n",
    "    start_dt = datetime.strptime(start_date, \"%Y-%m-%d\") if start_date else None\n",
    "    end_dt = datetime.strptime(end_date, \"%Y-%m-%d\") if end_date else None\n",
    "\n",
    "    # Get the filename inside the archive\n",
    "    with py7zr.SevenZipFile(archive_path, mode=\"r\") as archive:\n",
    "        file_list = archive.getnames()\n",
    "        if not file_list:\n",
    "            raise ValueError(\"No files found in archive\")\n",
    "        xml_filename = file_list[0]\n",
    "\n",
    "    # Create a temporary directory for chunk files\n",
    "    chunk_files = []\n",
    "    # with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    temp_dir = \"data/temp/\"\n",
    "    # Use 7z command-line tool to pipe the content without extraction\n",
    "    cmd = [\"7z\", \"e\", \"-so\", archive_path, xml_filename]\n",
    "    print(f\"Executing: {' '.join(cmd)}\")\n",
    "\n",
    "    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "    # Process XML in chunks with low memory footprint\n",
    "    context = ET.iterparse(\n",
    "        process.stdout,\n",
    "        events=(\"end\",),\n",
    "        tag=record_tag,\n",
    "        recover=True,\n",
    "        huge_tree=True,\n",
    "    )\n",
    "\n",
    "    all_data = []\n",
    "    total_processed = 0\n",
    "    total_skipped = 0\n",
    "    chunk_count = 0\n",
    "\n",
    "    # Keep track of all column names across all records\n",
    "    all_columns = set()\n",
    "\n",
    "    # Sample each record to determine the schema\n",
    "    # We'll use this later to ensure consistent types\n",
    "    schema_samples = {}\n",
    "\n",
    "    try:\n",
    "        for event, elem in context:\n",
    "            date_attr = elem.get(\"CreationDate\")\n",
    "\n",
    "            if date_attr:\n",
    "                date_str = date_attr\n",
    "                try:\n",
    "                    # Parse date\n",
    "                    if \"T\" in date_str:\n",
    "                        record_date = datetime.fromisoformat(\n",
    "                            date_str.replace(\"Z\", \"+00:00\")\n",
    "                        )\n",
    "                    else:\n",
    "                        record_date = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "\n",
    "                    # Apply date filtering\n",
    "                    date_in_range = True\n",
    "                    if start_dt and record_date.date() < start_dt.date():\n",
    "                        date_in_range = False\n",
    "                    if end_dt and record_date.date() > end_dt.date():\n",
    "                        date_in_range = False\n",
    "\n",
    "                    if date_in_range:\n",
    "                        row_data = dict(elem.attrib)\n",
    "\n",
    "                        # Collect schema samples (first non-null value for each column)\n",
    "                        for key, value in row_data.items():\n",
    "                            if key not in schema_samples and value is not None:\n",
    "                                schema_samples[key] = value\n",
    "\n",
    "                        # Update the set of all columns\n",
    "                        all_columns.update(row_data.keys())\n",
    "                        all_data.append(row_data)\n",
    "                        total_processed += 1\n",
    "                    else:\n",
    "                        total_skipped += 1\n",
    "                except (ValueError, TypeError) as e:\n",
    "                    print(\n",
    "                        f\"Skipping record with invalid date format: {date_str}, error: {e}\"\n",
    "                    )\n",
    "                    total_skipped += 1\n",
    "\n",
    "            # Clear memory\n",
    "            elem.clear()\n",
    "            while elem.getprevious() is not None:\n",
    "                del elem.getparent()[0]\n",
    "\n",
    "            # Write chunk to disk when it reaches batch size\n",
    "            if len(all_data) >= batch_size:\n",
    "                # Make sure all dictionaries have the same keys\n",
    "                for row in all_data:\n",
    "                    for col in all_columns:\n",
    "                        if col not in row:\n",
    "                            row[col] = None  # Add missing columns with None values\n",
    "                        # Convert all potential numeric fields to strings for consistency\n",
    "                        elif row[col] is not None:\n",
    "                            row[col] = str(row[col])\n",
    "\n",
    "                chunk_df = pl.from_dicts(all_data, infer_schema_length=1000000)\n",
    "                chunk_file = f\"{temp_dir}/chunk_{chunk_count}.parquet\"\n",
    "\n",
    "                # Convert all columns to string type for consistency\n",
    "                # This ensures we don't have type mismatches between chunks\n",
    "                for col in chunk_df.columns:\n",
    "                    if chunk_df[col].dtype != pl.String:\n",
    "                        chunk_df = chunk_df.with_columns(pl.col(col).cast(pl.String))\n",
    "\n",
    "                chunk_df.write_parquet(chunk_file)\n",
    "                chunk_files.append(chunk_file)\n",
    "\n",
    "                print(\n",
    "                    f\"Processed {total_processed} records, skipped {total_skipped}. Saved chunk {chunk_count}\"\n",
    "                )\n",
    "                chunk_count += 1\n",
    "                all_data = []  # Free memory\n",
    "\n",
    "        # Process remaining data\n",
    "        if all_data:\n",
    "            # Make sure all dictionaries have the same keys\n",
    "            for row in all_data:\n",
    "                for col in all_columns:\n",
    "                    if col not in row:\n",
    "                        row[col] = None  # Add missing columns with None values\n",
    "                    # Convert all potential numeric fields to strings for consistency\n",
    "                    elif row[col] is not None:\n",
    "                        row[col] = str(row[col])\n",
    "\n",
    "            chunk_df = pl.from_dicts(all_data, infer_schema_length=400000)\n",
    "            chunk_file = f\"{temp_dir}/chunk_{chunk_count}.parquet\"\n",
    "\n",
    "            # Convert all columns to string type for consistency\n",
    "            for col in chunk_df.columns:\n",
    "                if chunk_df[col].dtype != pl.String:\n",
    "                    chunk_df = chunk_df.with_columns(pl.col(col).cast(pl.String))\n",
    "\n",
    "            chunk_df.write_parquet(chunk_file)\n",
    "            chunk_files.append(chunk_file)\n",
    "\n",
    "        print(\n",
    "            f\"Completed. Total processed: {total_processed}, skipped: {total_skipped}\"\n",
    "        )\n",
    "\n",
    "        # Combine all chunks\n",
    "        if chunk_files:\n",
    "            # Read each parquet file with explicit schema control\n",
    "            dfs = []\n",
    "            schema = None\n",
    "\n",
    "            # First pass: determine a unified schema across all files\n",
    "            for file in chunk_files:\n",
    "                try:\n",
    "                    temp_schema = pl.read_parquet(file, n_rows=1).schema\n",
    "                    if schema is None:\n",
    "                        schema = temp_schema\n",
    "                    else:\n",
    "                        # Update schema to include all columns\n",
    "                        for col_name, dtype in temp_schema.items():\n",
    "                            if col_name not in schema:\n",
    "                                schema[col_name] = dtype\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading schema from {file}: {e}\")\n",
    "\n",
    "            # Second pass: read files with the unified schema\n",
    "            for file in chunk_files:\n",
    "                try:\n",
    "                    # Use the unified schema for all chunks\n",
    "                    temp_df = pl.read_parquet(file)\n",
    "\n",
    "                    # Ensure all columns from the schema exist\n",
    "                    for col_name in schema.keys():\n",
    "                        if col_name not in temp_df.columns:\n",
    "                            # Add missing columns with null values\n",
    "                            temp_df = temp_df.with_columns(pl.lit(None).alias(col_name))\n",
    "\n",
    "                    # Cast all columns to string to ensure type consistency\n",
    "                    for col in temp_df.columns:\n",
    "                        temp_df = temp_df.with_columns(pl.col(col).cast(pl.String))\n",
    "\n",
    "                    dfs.append(temp_df)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "            # Combine all DataFrames with vertical concatenation\n",
    "            if dfs:\n",
    "                df = pl.concat(dfs)\n",
    "                print(\n",
    "                    f\"Created final DataFrame with {len(df)} rows and {len(df.columns)} columns\"\n",
    "                )\n",
    "            else:\n",
    "                print(\"Failed to read any chunks\")\n",
    "                df = pl.DataFrame()\n",
    "\n",
    "            # Save the final result\n",
    "            output_file = \"stackoverflow_filtered_data.parquet\"\n",
    "            df.write_parquet(output_file)\n",
    "            print(f\"Data saved to {output_file}\")\n",
    "\n",
    "            return df\n",
    "        else:\n",
    "            print(\"No data matched the criteria\")\n",
    "            return pl.DataFrame()\n",
    "\n",
    "    finally:\n",
    "        # Terminate the subprocess if still running\n",
    "        if process.poll() is None:\n",
    "            process.terminate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing: 7z e -so data/stackoverflow.com-Posts.7z Posts.xml\n",
      "Processed 100000 records, skipped 59433515. Saved chunk 0\n",
      "Processed 200000 records, skipped 59433515. Saved chunk 1\n",
      "Processed 300000 records, skipped 59433515. Saved chunk 2\n",
      "Completed. Total processed: 385533, skipped: 59433515\n"
     ]
    },
    {
     "ename": "ShapeError",
     "evalue": "unable to vstack, column names don't match: \"AcceptedAnswerId\" and \"ParentId\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mShapeError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Use the chunked approach which is most memory and disk efficient\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df = \u001b[43mchunked_process_xml_in_7z\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata/stackoverflow.com-Posts.7z\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdate_xpath\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.//CreationDate\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m2024-01-01\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m2024-12-31\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrecord_tag\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrow\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Display the first few rows\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m df.is_empty():\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 353\u001b[39m, in \u001b[36mchunked_process_xml_in_7z\u001b[39m\u001b[34m(archive_path, batch_size, date_xpath, start_date, end_date, record_tag)\u001b[39m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# Combine all DataFrames with vertical concatenation\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dfs:\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m     df = \u001b[43mpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdfs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    354\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    355\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCreated final DataFrame with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rows and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df.columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m columns\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    356\u001b[39m     )\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/documents/bse/trimester 2/textmining/bse_tm_final/.venv/lib/python3.12/site-packages/polars/functions/eager.py:229\u001b[39m, in \u001b[36mconcat\u001b[39m\u001b[34m(items, how, rechunk, parallel)\u001b[39m\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(first, pl.DataFrame):\n\u001b[32m    228\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m how == \u001b[33m\"\u001b[39m\u001b[33mvertical\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m229\u001b[39m         out = wrap_df(\u001b[43mplr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcat_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43melems\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    230\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m how == \u001b[33m\"\u001b[39m\u001b[33mvertical_relaxed\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    231\u001b[39m         out = wrap_ldf(\n\u001b[32m    232\u001b[39m             plr.concat_lf(\n\u001b[32m    233\u001b[39m                 [df.lazy() \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m elems],\n\u001b[32m   (...)\u001b[39m\u001b[32m    237\u001b[39m             )\n\u001b[32m    238\u001b[39m         ).collect(no_optimization=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mShapeError\u001b[39m: unable to vstack, column names don't match: \"AcceptedAnswerId\" and \"ParentId\""
     ]
    }
   ],
   "source": [
    "# Use the chunked approach which is most memory and disk efficient\n",
    "df = chunked_process_xml_in_7z(\n",
    "    \"data/stackoverflow.com-Posts.7z\",\n",
    "    batch_size=100000,\n",
    "    date_xpath=\".//CreationDate\",\n",
    "    start_date=\"2024-01-01\",\n",
    "    end_date=\"2024-12-31\",\n",
    "    record_tag=\"row\",\n",
    ")\n",
    "\n",
    "# Display the first few rows\n",
    "if not df.is_empty():\n",
    "    print(\"\\nDataFrame Preview:\")\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = pl.read_parquet(\"data/temp/chunk_0.parquet\")\n",
    "df1 = pl.read_parquet(\"data/temp/chunk_1.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 22)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 22)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pl.read_parquet(\"data/temp/chunk_2.parquet\")\n",
    "df3 = pl.read_parquet(\"data/temp/chunk_3.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 22)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85533, 22)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "\n",
    "def process_xml_in_7z_with_7z_executable(\n",
    "    archive_path,\n",
    "    batch_size=1000,\n",
    "    date_xpath=\".//CreationDate\",\n",
    "    start_date=None,\n",
    "    end_date=None,\n",
    "    record_tag=\"row\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Process an XML file within a 7z archive using the 7z command line tool to pipe data.\n",
    "    This approach avoids extracting to disk.\n",
    "\n",
    "    Args:\n",
    "        archive_path (str): Path to the .7z archive\n",
    "        batch_size (int): Number of elements to process in each batch\n",
    "        date_xpath (str): XPath to the date element within each record\n",
    "        start_date (str): Optional start date in format 'YYYY-MM-DD'\n",
    "        end_date (str): Optional end date in format 'YYYY-MM-DD'\n",
    "        record_tag (str): XML tag name for records to process\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: Polars DataFrame containing the processed data\n",
    "    \"\"\"\n",
    "    # Convert date strings to datetime objects if provided\n",
    "    start_dt = datetime.strptime(start_date, \"%Y-%m-%d\") if start_date else None\n",
    "    end_dt = datetime.strptime(end_date, \"%Y-%m-%d\") if end_date else None\n",
    "\n",
    "    # Get the filename inside the archive\n",
    "    with py7zr.SevenZipFile(archive_path, mode=\"r\") as archive:\n",
    "        file_list = archive.getnames()\n",
    "        if not file_list:\n",
    "            raise ValueError(\"No files found in archive\")\n",
    "        xml_filename = file_list[0]\n",
    "\n",
    "    # Use 7z command-line tool to pipe the content without extraction\n",
    "    cmd = [\"7z\", \"e\", \"-so\", archive_path, xml_filename]\n",
    "    print(f\"Executing: {' '.join(cmd)}\")\n",
    "\n",
    "    # Initialize data lists\n",
    "    all_data = []\n",
    "    batch = []\n",
    "    total_processed = 0\n",
    "    total_skipped = 0\n",
    "\n",
    "    # Create a subprocess to stream the data\n",
    "    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "    # Use lxml's iterparse to process the stream\n",
    "    context = ET.iterparse(\n",
    "        process.stdout, events=(\"end\",), tag=record_tag, recover=True, huge_tree=True\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        for event, elem in context:\n",
    "            # Extract date value from attributes (common in StackOverflow XML)\n",
    "            date_attr = elem.get(\"CreationDate\")\n",
    "\n",
    "            if date_attr:\n",
    "                date_str = date_attr\n",
    "                try:\n",
    "                    # Handle ISO format dates (common in StackOverflow)\n",
    "                    if \"T\" in date_str:\n",
    "                        record_date = datetime.fromisoformat(\n",
    "                            date_str.replace(\"Z\", \"+00:00\")\n",
    "                        )\n",
    "                    else:\n",
    "                        record_date = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "\n",
    "                    # Apply date filtering\n",
    "                    date_in_range = True\n",
    "                    if start_dt and record_date.date() < start_dt.date():\n",
    "                        date_in_range = False\n",
    "                    if end_dt and record_date.date() > end_dt.date():\n",
    "                        date_in_range = False\n",
    "\n",
    "                    if date_in_range:\n",
    "                        # Extract all attributes into a dictionary\n",
    "                        row_data = dict(elem.attrib)\n",
    "                        batch.append(row_data)\n",
    "                    else:\n",
    "                        total_skipped += 1\n",
    "                except (ValueError, TypeError) as e:\n",
    "                    print(\n",
    "                        f\"Skipping record with invalid date format: {date_str}, error: {e}\"\n",
    "                    )\n",
    "                    total_skipped += 1\n",
    "\n",
    "            # Process in batches\n",
    "            if len(batch) >= batch_size:\n",
    "                all_data.extend(batch)\n",
    "                total_processed += len(batch)\n",
    "                print(\n",
    "                    f\"Processed {total_processed} records, skipped {total_skipped} records\"\n",
    "                )\n",
    "                batch = []\n",
    "\n",
    "            # Clear the element from memory\n",
    "            elem.clear()\n",
    "            # Also eliminate now-empty references from the root node to elem\n",
    "            while elem.getprevious() is not None:\n",
    "                del elem.getparent()[0]\n",
    "\n",
    "            # Every 10,000 records, convert current data to DataFrame to free memory\n",
    "            if total_processed > 0 and total_processed % 10000 == 0:\n",
    "                if all_data:\n",
    "                    # Convert to polars dataframe\n",
    "                    temp_df = pl.from_dicts(all_data, infer_schema_length=100000)\n",
    "                    if \"df\" not in locals():\n",
    "                        df = temp_df\n",
    "                    else:\n",
    "                        # Append to existing dataframe\n",
    "                        df = pl.concat([df, temp_df])\n",
    "                    # Clear the all_data list to free memory\n",
    "                    all_data = []\n",
    "\n",
    "    finally:\n",
    "        # Terminate the subprocess if it's still running\n",
    "        if process.poll() is None:\n",
    "            process.terminate()\n",
    "\n",
    "    # Process any remaining items\n",
    "    if batch:\n",
    "        all_data.extend(batch)\n",
    "        total_processed += len(batch)\n",
    "\n",
    "    print(f\"Completed. Total processed: {total_processed}, skipped: {total_skipped}\")\n",
    "\n",
    "    # Convert remaining data to DataFrame\n",
    "    if all_data:\n",
    "        temp_df = pl.from_dicts(all_data, infer_schema_length=100000)\n",
    "        if \"df\" in locals():\n",
    "            df = pl.concat([df, temp_df])\n",
    "        else:\n",
    "            df = temp_df\n",
    "\n",
    "    if \"df\" in locals() and not df.is_empty():\n",
    "        print(\n",
    "            f\"Created Polars DataFrame with {len(df)} rows and {len(df.columns)} columns\"\n",
    "        )\n",
    "        return df\n",
    "    else:\n",
    "        print(\"No data matched the criteria\")\n",
    "        return pl.DataFrame()\n",
    "\n",
    "\n",
    "def chunked_process_xml_in_7z(\n",
    "    archive_path,\n",
    "    batch_size=1000,\n",
    "    start_date=None,\n",
    "    end_date=None,\n",
    "    record_tag=\"row\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Process an XML file within a 7z archive in chunks, writing intermediate results to parquet.\n",
    "    This approach uses very little memory and ensures consistent column order across chunks.\n",
    "    \"\"\"\n",
    "    # Convert date strings to datetime objects if provided\n",
    "    start_dt = datetime.strptime(start_date, \"%Y-%m-%d\") if start_date else None\n",
    "    end_dt = datetime.strptime(end_date, \"%Y-%m-%d\") if end_date else None\n",
    "\n",
    "    # Get the filename inside the archive\n",
    "    with py7zr.SevenZipFile(archive_path, mode=\"r\") as archive:\n",
    "        file_list = archive.getnames()\n",
    "        if not file_list:\n",
    "            raise ValueError(\"No files found in archive\")\n",
    "        xml_filename = file_list[0]\n",
    "\n",
    "    # Create a temporary directory for chunk files\n",
    "    chunk_files = []\n",
    "    temp_dir = \"data/temp/\"\n",
    "\n",
    "    # Use 7z command-line tool to pipe the content without extraction\n",
    "    cmd = [\"7z\", \"e\", \"-so\", archive_path, xml_filename]\n",
    "    print(f\"Executing: {' '.join(cmd)}\")\n",
    "\n",
    "    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "    # Process XML in chunks with low memory footprint\n",
    "    context = ET.iterparse(\n",
    "        process.stdout,\n",
    "        events=(\"end\",),\n",
    "        tag=record_tag,\n",
    "        recover=True,\n",
    "        huge_tree=True,\n",
    "    )\n",
    "\n",
    "    all_data = []\n",
    "    total_processed = 0\n",
    "    total_skipped = 0\n",
    "    chunk_count = 0\n",
    "\n",
    "    # Track all columns across all records and maintain a consistent order\n",
    "    all_columns = set()\n",
    "\n",
    "    try:\n",
    "        # First pass - scan the beginning to determine most likely columns\n",
    "        # This helps establish a baseline schema\n",
    "        preview_count = 0\n",
    "        preview_limit = min(\n",
    "            1000, batch_size\n",
    "        )  # Preview first 1000 records or batch_size\n",
    "        preview_data = []\n",
    "\n",
    "        for event, elem in context:\n",
    "            if preview_count >= preview_limit:\n",
    "                break\n",
    "\n",
    "            date_attr = elem.get(\"CreationDate\")\n",
    "            if date_attr:\n",
    "                try:\n",
    "                    # Parse date for filtering\n",
    "                    if \"T\" in date_attr:\n",
    "                        record_date = datetime.fromisoformat(\n",
    "                            date_attr.replace(\"Z\", \"+00:00\")\n",
    "                        )\n",
    "                    else:\n",
    "                        record_date = datetime.strptime(date_attr, \"%Y-%m-%d\")\n",
    "\n",
    "                    # Apply date filtering\n",
    "                    date_in_range = True\n",
    "                    if start_dt and record_date.date() < start_dt.date():\n",
    "                        date_in_range = False\n",
    "                    if end_dt and record_date.date() > end_dt.date():\n",
    "                        date_in_range = False\n",
    "\n",
    "                    if date_in_range:\n",
    "                        row_data = dict(elem.attrib)\n",
    "                        preview_data.append(row_data)\n",
    "                        # Update the set of all columns\n",
    "                        all_columns.update(row_data.keys())\n",
    "                        preview_count += 1\n",
    "                except (ValueError, TypeError):\n",
    "                    pass\n",
    "\n",
    "            # Clear memory\n",
    "            elem.clear()\n",
    "            while elem.getprevious() is not None:\n",
    "                del elem.getparent()[0]\n",
    "\n",
    "        # Convert to ordered list to maintain consistent column order\n",
    "        ordered_columns = sorted(list(all_columns))\n",
    "        print(f\"Established initial schema with {len(ordered_columns)} columns\")\n",
    "\n",
    "        # Reset the process to start from the beginning\n",
    "        process.terminate()\n",
    "        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        context = ET.iterparse(\n",
    "            process.stdout,\n",
    "            events=(\"end\",),\n",
    "            tag=record_tag,\n",
    "            recover=True,\n",
    "            huge_tree=True,\n",
    "        )\n",
    "\n",
    "        # Now process the full file with established schema\n",
    "        for event, elem in context:\n",
    "            date_attr = elem.get(\"CreationDate\")\n",
    "\n",
    "            if date_attr:\n",
    "                date_str = date_attr\n",
    "                try:\n",
    "                    # Parse date\n",
    "                    if \"T\" in date_str:\n",
    "                        record_date = datetime.fromisoformat(\n",
    "                            date_str.replace(\"Z\", \"+00:00\")\n",
    "                        )\n",
    "                    else:\n",
    "                        record_date = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "\n",
    "                    # Apply date filtering\n",
    "                    date_in_range = True\n",
    "                    if start_dt and record_date.date() < start_dt.date():\n",
    "                        date_in_range = False\n",
    "                    if end_dt and record_date.date() > end_dt.date():\n",
    "                        date_in_range = False\n",
    "\n",
    "                    if date_in_range:\n",
    "                        # Get all attributes and ensure string values for consistency\n",
    "                        row_data = {\n",
    "                            k: str(v) if v is not None else None\n",
    "                            for k, v in elem.attrib.items()\n",
    "                        }\n",
    "\n",
    "                        # Add this row's columns to our master list if new ones appear\n",
    "                        new_columns = set(row_data.keys()) - set(ordered_columns)\n",
    "                        if new_columns:\n",
    "                            ordered_columns.extend(sorted(new_columns))\n",
    "\n",
    "                        all_data.append(row_data)\n",
    "                        total_processed += 1\n",
    "                    else:\n",
    "                        total_skipped += 1\n",
    "                except (ValueError, TypeError) as e:\n",
    "                    print(\n",
    "                        f\"Skipping record with invalid date format: {date_str}, error: {e}\"\n",
    "                    )\n",
    "                    total_skipped += 1\n",
    "\n",
    "            # Clear memory\n",
    "            elem.clear()\n",
    "            while elem.getprevious() is not None:\n",
    "                del elem.getparent()[0]\n",
    "\n",
    "            # Write chunk to disk when it reaches batch size\n",
    "            if len(all_data) >= batch_size:\n",
    "                # Create DataFrame with normalized structure\n",
    "                chunk_df = pl.from_dicts(all_data, infer_schema_length=100000)\n",
    "\n",
    "                # Add any missing columns from our master list\n",
    "                missing_columns = [\n",
    "                    col for col in ordered_columns if col not in chunk_df.columns\n",
    "                ]\n",
    "                for col in missing_columns:\n",
    "                    chunk_df = chunk_df.with_columns(\n",
    "                        pl.lit(None).cast(pl.String).alias(col)\n",
    "                    )\n",
    "\n",
    "                # Ensure all columns are strings for consistency\n",
    "                chunk_df = chunk_df.select(\n",
    "                    [pl.col(col).cast(pl.String) for col in chunk_df.columns]\n",
    "                )\n",
    "\n",
    "                # Reorder columns to match our standard order\n",
    "                available_columns = [\n",
    "                    col for col in ordered_columns if col in chunk_df.columns\n",
    "                ]\n",
    "                chunk_df = chunk_df.select(available_columns)\n",
    "\n",
    "                # Write to parquet\n",
    "                chunk_file = f\"{temp_dir}/chunk_{chunk_count}.parquet\"\n",
    "                chunk_df.write_parquet(chunk_file)\n",
    "                chunk_files.append(chunk_file)\n",
    "\n",
    "                print(\n",
    "                    f\"Processed {total_processed} records, skipped {total_skipped}. Saved chunk {chunk_count}\"\n",
    "                )\n",
    "                chunk_count += 1\n",
    "                all_data = []  # Free memory\n",
    "\n",
    "        # Process remaining data\n",
    "        if all_data:\n",
    "            # Create DataFrame with normalized structure\n",
    "            chunk_df = pl.from_dicts(all_data, infer_schema_length=100000)\n",
    "\n",
    "            # Add any missing columns from our master list\n",
    "            missing_columns = [\n",
    "                col for col in ordered_columns if col not in chunk_df.columns\n",
    "            ]\n",
    "            for col in missing_columns:\n",
    "                chunk_df = chunk_df.with_columns(\n",
    "                    pl.lit(None).cast(pl.String).alias(col)\n",
    "                )\n",
    "\n",
    "            # Ensure all columns are strings for consistency\n",
    "            chunk_df = chunk_df.select(\n",
    "                [pl.col(col).cast(pl.String) for col in chunk_df.columns]\n",
    "            )\n",
    "\n",
    "            # Reorder columns to match our standard order\n",
    "            available_columns = [\n",
    "                col for col in ordered_columns if col in chunk_df.columns\n",
    "            ]\n",
    "            chunk_df = chunk_df.select(available_columns)\n",
    "\n",
    "            # Write to parquet\n",
    "            chunk_file = f\"{temp_dir}/chunk_{chunk_count}.parquet\"\n",
    "            chunk_df.write_parquet(chunk_file)\n",
    "            chunk_files.append(chunk_file)\n",
    "\n",
    "        print(\n",
    "            f\"Completed. Total processed: {total_processed}, skipped: {total_skipped}\"\n",
    "        )\n",
    "\n",
    "        # Combine all chunks\n",
    "        if chunk_files:\n",
    "            # Read and combine all parquet files\n",
    "            dfs = []\n",
    "\n",
    "            for file in chunk_files:\n",
    "                try:\n",
    "                    # Read the parquet file\n",
    "                    temp_df = pl.read_parquet(file)\n",
    "                    dfs.append(temp_df)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "            # Combine all DataFrames with vertical concatenation\n",
    "            if dfs:\n",
    "                # All DataFrames should have the same column order now\n",
    "                df = pl.concat(dfs)\n",
    "\n",
    "                # Final safety check - ensure all columns have same type\n",
    "                df = df.select([pl.col(col).cast(pl.String) for col in df.columns])\n",
    "\n",
    "                print(\n",
    "                    f\"Created final DataFrame with {len(df)} rows and {len(df.columns)} columns\"\n",
    "                )\n",
    "            else:\n",
    "                print(\"Failed to read any chunks\")\n",
    "                df = pl.DataFrame()\n",
    "\n",
    "            # Save the final result\n",
    "            output_file = \"stackoverflow_filtered_data.parquet\"\n",
    "            df.write_parquet(output_file)\n",
    "            print(f\"Data saved to {output_file}\")\n",
    "\n",
    "            return df\n",
    "        else:\n",
    "            print(\"No data matched the criteria\")\n",
    "            return pl.DataFrame()\n",
    "\n",
    "    finally:\n",
    "        # Terminate the subprocess if still running\n",
    "        if process.poll() is None:\n",
    "            process.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing: 7z e -so data/stackoverflow.com-Posts.7z Posts.xml\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Use the chunked approach which is most memory and disk efficient\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df = \u001b[43mchunked_process_xml_in_7z\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata/stackoverflow.com-Posts.7z\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m2021-01-01\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m2024-12-31\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrecord_tag\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrow\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Display the first few rows\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m df.is_empty():\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 217\u001b[39m, in \u001b[36mchunked_process_xml_in_7z\u001b[39m\u001b[34m(archive_path, batch_size, start_date, end_date, record_tag)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m preview_count >= preview_limit:\n\u001b[32m    215\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m date_attr = \u001b[43melem\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCreationDate\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m date_attr:\n\u001b[32m    219\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    220\u001b[39m         \u001b[38;5;66;03m# Parse date for filtering\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Use the chunked approach which is most memory and disk efficient\n",
    "df = chunked_process_xml_in_7z(\n",
    "    \"data/stackoverflow.com-Posts.7z\",\n",
    "    batch_size=100000,\n",
    "    start_date=\"2021-01-01\",\n",
    "    end_date=\"2024-12-31\",\n",
    "    record_tag=\"row\",\n",
    ")\n",
    "\n",
    "# Display the first few rows\n",
    "if not df.is_empty():\n",
    "    print(\"\\nDataFrame Preview:\")\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = []\n",
    "# List files in directory\n",
    "files = os.listdir(\"data/temp/\")\n",
    "for file in files:\n",
    "    if file.endswith(\".parquet\"):\n",
    "        chunk = pl.read_parquet(f\"data/temp/{file}\")\n",
    "        chunks.append(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000000, 21)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pl.concat(chunks, how=\"diagonal\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 21)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Id</th><th>PostTypeId</th><th>CreationDate</th><th>Score</th><th>ViewCount</th><th>Body</th><th>OwnerUserId</th><th>LastActivityDate</th><th>Title</th><th>Tags</th><th>AnswerCount</th><th>CommentCount</th><th>ContentLicense</th><th>AcceptedAnswerId</th><th>LastEditorUserId</th><th>LastEditDate</th><th>ClosedDate</th><th>OwnerDisplayName</th><th>FavoriteCount</th><th>LastEditorDisplayName</th><th>CommunityOwnedDate</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;66489236&quot;</td><td>&quot;1&quot;</td><td>&quot;2021-03-05T08:35:32.080&quot;</td><td>&quot;2&quot;</td><td>&quot;261&quot;</td><td>&quot;&lt;p&gt;package.json file&lt;/p&gt;\n",
       "&lt;pre&gt;</td><td>&quot;15252063&quot;</td><td>&quot;2021-03-05T16:00:44.680&quot;</td><td>&quot;is it possible to do sql injec</td><td>&quot;|sql|node.js|postgresql|web|gr</td><td>&quot;1&quot;</td><td>&quot;0&quot;</td><td>&quot;CC BY-SA 4.0&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;66489239&quot;</td><td>&quot;1&quot;</td><td>&quot;2021-03-05T08:35:53.420&quot;</td><td>&quot;0&quot;</td><td>&quot;1188&quot;</td><td>&quot;&lt;p&gt;I tried to follow as many a</td><td>&quot;1013139&quot;</td><td>&quot;2021-03-10T02:57:17.007&quot;</td><td>&quot;iOS applink not working on the</td><td>&quot;|ios|deep-linking|applinks|ios</td><td>&quot;1&quot;</td><td>&quot;0&quot;</td><td>&quot;CC BY-SA 4.0&quot;</td><td>&quot;66557704&quot;</td><td>&quot;1013139&quot;</td><td>&quot;2021-03-08T23:28:52.147&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;66509681&quot;</td><td>&quot;1&quot;</td><td>&quot;2021-03-06T19:12:58.427&quot;</td><td>&quot;0&quot;</td><td>&quot;380&quot;</td><td>&quot;&lt;p&gt;I&#x27;m trying to figure out ho</td><td>&quot;12913047&quot;</td><td>&quot;2021-03-06T19:25:49.643&quot;</td><td>&quot;How to create utility color cl</td><td>&quot;|css|&quot;</td><td>&quot;1&quot;</td><td>&quot;2&quot;</td><td>&quot;CC BY-SA 4.0&quot;</td><td>&quot;66509790&quot;</td><td>&quot;12913047&quot;</td><td>&quot;2021-03-06T19:16:18.500&quot;</td><td>&quot;2021-03-06T19:27:27.250&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;66509682&quot;</td><td>&quot;1&quot;</td><td>&quot;2021-03-06T19:12:59.847&quot;</td><td>&quot;1&quot;</td><td>&quot;100&quot;</td><td>&quot;&lt;p&gt;I&#x27;m trying to run an evalua</td><td>&quot;9443671&quot;</td><td>&quot;2021-03-06T19:12:59.847&quot;</td><td>&quot;Running into troubles with dat</td><td>&quot;|directory|path|bert-language-</td><td>&quot;0&quot;</td><td>&quot;1&quot;</td><td>&quot;CC BY-SA 4.0&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;66459802&quot;</td><td>&quot;1&quot;</td><td>&quot;2021-03-03T15:12:43.877&quot;</td><td>&quot;0&quot;</td><td>&quot;11&quot;</td><td>&quot;&lt;p&gt;Hello I have a dataframe su</td><td>&quot;12559770&quot;</td><td>&quot;2021-03-03T15:12:43.877&quot;</td><td>&quot;Add new column with the colnam</td><td>&quot;|r|dplyr|&quot;</td><td>&quot;0&quot;</td><td>&quot;4&quot;</td><td>&quot;CC BY-SA 4.0&quot;</td><td>null</td><td>null</td><td>null</td><td>&quot;2021-03-03T15:14:14.910&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 21)\n",
       "\n",
       " Id        PostTypeId  CreationDa  Score    OwnerDispl  FavoriteCo  LastEdito  Community \n",
       " ---       ---         te          ---       ayName      unt         rDisplayN  OwnedDate \n",
       " str       str         ---         str       ---         ---         ame        ---       \n",
       "                       str                   str         str         ---        str       \n",
       "                                                                     str                  \n",
       "\n",
       " 66489236  1           2021-03-05  2        null        null        null       null      \n",
       "                       T08:35:32.                                                         \n",
       "                       080                                                                \n",
       " 66489239  1           2021-03-05  0        null        null        null       null      \n",
       "                       T08:35:53.                                                         \n",
       "                       420                                                                \n",
       " 66509681  1           2021-03-06  0        null        null        null       null      \n",
       "                       T19:12:58.                                                         \n",
       "                       427                                                                \n",
       " 66509682  1           2021-03-06  1        null        null        null       null      \n",
       "                       T19:12:59.                                                         \n",
       "                       847                                                                \n",
       " 66459802  1           2021-03-03  0        null        null        null       null      \n",
       "                       T15:12:43.                                                         \n",
       "                       877                                                                \n",
       ""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.with_columns(\n",
    "    pl.col(\"CreationDate\").cast(pl.Datetime),\n",
    "    pl.col(\"LastActivityDate\").cast(pl.Datetime),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write_parquet(\"stackoverflow_filtered_data1.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2498557, 22)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pl.read_parquet(\"stackoverflow_filtered_data.parquet\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-ddf5042f4b344b43a1233892e8493547.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-ddf5042f4b344b43a1233892e8493547.vega-embed details,\n",
       "  #altair-viz-ddf5042f4b344b43a1233892e8493547.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-ddf5042f4b344b43a1233892e8493547\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-ddf5042f4b344b43a1233892e8493547\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-ddf5042f4b344b43a1233892e8493547\");\n",
       "    }\n",
       "\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      let deps = [\"vega-embed\"];\n",
       "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-198ba1523f7a11a6c147b91e093bfded\"}, \"mark\": {\"type\": \"line\", \"tooltip\": true}, \"encoding\": {\"x\": {\"field\": \"YM\", \"type\": \"nominal\"}, \"y\": {\"field\": \"Count\", \"type\": \"quantitative\"}}, \"params\": [{\"name\": \"param_7\", \"select\": {\"type\": \"interval\", \"encodings\": [\"x\", \"y\"]}, \"bind\": \"scales\"}], \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-198ba1523f7a11a6c147b91e093bfded\": [{\"YM\": \"2021-01\", \"Count\": 140648}, {\"YM\": \"2021-02\", \"Count\": 132462}, {\"YM\": \"2021-03\", \"Count\": 149693}, {\"YM\": \"2021-04\", \"Count\": 136784}, {\"YM\": \"2021-05\", \"Count\": 134669}, {\"YM\": \"2021-06\", \"Count\": 129848}, {\"YM\": \"2021-07\", \"Count\": 124844}, {\"YM\": \"2021-08\", \"Count\": 123009}, {\"YM\": \"2021-09\", \"Count\": 120581}, {\"YM\": \"2021-10\", \"Count\": 119730}, {\"YM\": \"2021-11\", \"Count\": 120049}, {\"YM\": \"2021-12\", \"Count\": 113061}, {\"YM\": \"2022-01\", \"Count\": 120294}, {\"YM\": \"2022-02\", \"Count\": 114868}, {\"YM\": \"2022-03\", \"Count\": 124603}, {\"YM\": \"2022-04\", \"Count\": 115348}, {\"YM\": \"2022-05\", \"Count\": 117258}, {\"YM\": \"2022-06\", \"Count\": 112584}, {\"YM\": \"2022-07\", \"Count\": 111927}, {\"YM\": \"2022-08\", \"Count\": 113982}, {\"YM\": \"2022-09\", \"Count\": 104743}, {\"YM\": \"2022-10\", \"Count\": 107297}, {\"YM\": \"2022-11\", \"Count\": 110750}, {\"YM\": \"2022-12\", \"Count\": 97715}, {\"YM\": \"2023-01\", \"Count\": 98254}, {\"YM\": \"2023-02\", \"Count\": 86957}, {\"YM\": \"2023-03\", \"Count\": 88732}, {\"YM\": \"2023-04\", \"Count\": 84784}, {\"YM\": \"2023-05\", \"Count\": 86755}, {\"YM\": \"2023-06\", \"Count\": 84203}, {\"YM\": \"2023-07\", \"Count\": 82892}, {\"YM\": \"2023-08\", \"Count\": 79084}, {\"YM\": \"2023-09\", \"Count\": 69182}, {\"YM\": \"2023-10\", \"Count\": 70410}, {\"YM\": \"2023-11\", \"Count\": 66717}, {\"YM\": \"2023-12\", \"Count\": 56607}, {\"YM\": \"2024-01\", \"Count\": 63183}, {\"YM\": \"2024-02\", \"Count\": 61682}, {\"YM\": \"2024-03\", \"Count\": 23811}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_plot = (\n",
    "    df.with_columns(pl.col(\"CreationDate\").dt.strftime(\"%Y-%m\").alias(\"YM\"))\n",
    "    # .filter(pl.col(\"Tags\").str.contains(\"java\"))\n",
    "    .group_by(\"YM\")\n",
    "    .agg(pl.len().alias(\"Count\"))\n",
    "    .sort(\"YM\")\n",
    ")\n",
    "df_plot.plot.line(x=\"YM\", y=\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (16, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>day</th><th>count</th></tr><tr><td>str</td><td>u32</td></tr></thead><tbody><tr><td>&quot;12&quot;</td><td>884</td></tr><tr><td>&quot;14&quot;</td><td>1723</td></tr><tr><td>&quot;15&quot;</td><td>282</td></tr><tr><td>&quot;10&quot;</td><td>1087</td></tr><tr><td>&quot;07&quot;</td><td>2778</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;16&quot;</td><td>72</td></tr><tr><td>&quot;05&quot;</td><td>2351</td></tr><tr><td>&quot;13&quot;</td><td>926</td></tr><tr><td>&quot;03&quot;</td><td>1196</td></tr><tr><td>&quot;09&quot;</td><td>1249</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (16, 2)\n",
       "\n",
       " day  count \n",
       " ---  ---   \n",
       " str  u32   \n",
       "\n",
       " 12   884   \n",
       " 14   1723  \n",
       " 15   282   \n",
       " 10   1087  \n",
       " 07   2778  \n",
       "          \n",
       " 16   72    \n",
       " 05   2351  \n",
       " 13   926   \n",
       " 03   1196  \n",
       " 09   1249  \n",
       ""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.with_columns(\n",
    "    pl.col(\"CreationDate\").dt.strftime(\"%Y-%m\").alias(\"month\"),\n",
    "    pl.col(\"CreationDate\").dt.strftime(\"%d\").alias(\"day\"),\n",
    ").filter(pl.col(\"month\") == \"2024-03\")[\"day\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "\n",
    "def process_xml_in_7z(\n",
    "    archive_path,\n",
    "    batch_size=1000,\n",
    "    start_date=None,\n",
    "    end_date=None,\n",
    "    record_tag=\"row\",\n",
    "    chunk_to_disk=False,\n",
    "    temp_dir=\"data/temp/\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Process an XML file within a 7z archive efficiently, filtering for non-empty titles.\n",
    "\n",
    "    Args:\n",
    "        archive_path (str): Path to the .7z archive\n",
    "        batch_size (int): Number of elements to process in each batch\n",
    "        start_date (str): Optional start date in format 'YYYY-MM-DD'\n",
    "        end_date (str): Optional end date in format 'YYYY-MM-DD'\n",
    "        record_tag (str): XML tag name for records to process\n",
    "        chunk_to_disk (bool): Whether to write intermediate chunks to disk (for very large files)\n",
    "        temp_dir (str): Directory to store temporary chunk files if chunking is enabled\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: Polars DataFrame containing the processed data\n",
    "    \"\"\"\n",
    "    # Convert date strings to datetime objects if provided\n",
    "    start_dt = datetime.strptime(start_date, \"%Y-%m-%d\") if start_date else None\n",
    "    end_dt = datetime.strptime(end_date, \"%Y-%m-%d\") if end_date else None\n",
    "\n",
    "    # Get the filename inside the archive\n",
    "    with py7zr.SevenZipFile(archive_path, mode=\"r\") as archive:\n",
    "        file_list = archive.getnames()\n",
    "        if not file_list:\n",
    "            raise ValueError(\"No files found in archive\")\n",
    "        xml_filename = \"Posts.xml\"  # file_list[0]\n",
    "\n",
    "    # Use 7z command-line tool to pipe the content without extraction\n",
    "    cmd = [\"7z\", \"e\", \"-so\", archive_path, xml_filename]\n",
    "    print(f\"Executing: {' '.join(cmd)}\")\n",
    "\n",
    "    # Initialize tracking variables\n",
    "    all_data = []\n",
    "    total_processed = 0\n",
    "    total_skipped = 0\n",
    "    chunk_files = []\n",
    "    chunk_count = 0\n",
    "    ordered_columns = set()\n",
    "\n",
    "    # Create temp directory if chunking is enabled\n",
    "    if chunk_to_disk:\n",
    "        Path(temp_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Start the extraction process\n",
    "    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "    # Process XML with iterparse\n",
    "    context = ET.iterparse(\n",
    "        process.stdout, events=(\"end\",), tag=record_tag, recover=True, huge_tree=True\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        for _, elem in context:\n",
    "            # Check for title first (our primary filter)\n",
    "            title = elem.get(\"Title\")\n",
    "            if not title or title.strip() == \"\":\n",
    "                total_skipped += 1\n",
    "                elem.clear()\n",
    "                continue\n",
    "\n",
    "            # Extract date if we also need date filtering\n",
    "            date_attr = elem.get(\"CreationDate\")\n",
    "\n",
    "            # Apply date filtering if needed\n",
    "            if start_dt or end_dt:\n",
    "                if date_attr:\n",
    "                    try:\n",
    "                        # Parse date for filtering\n",
    "                        if \"T\" in date_attr:\n",
    "                            record_date = datetime.fromisoformat(\n",
    "                                date_attr.replace(\"Z\", \"+00:00\")\n",
    "                            )\n",
    "                        else:\n",
    "                            record_date = datetime.strptime(date_attr, \"%Y-%m-%d\")\n",
    "\n",
    "                        # Skip if out of date range\n",
    "                        if (start_dt and record_date.date() < start_dt.date()) or (\n",
    "                            end_dt and record_date.date() > end_dt.date()\n",
    "                        ):\n",
    "                            total_skipped += 1\n",
    "                            elem.clear()\n",
    "                            continue\n",
    "                    except (ValueError, TypeError) as e:\n",
    "                        print(f\"Warning: Invalid date format '{date_attr}', error: {e}\")\n",
    "                        # Continue processing the record anyway since title is the primary filter\n",
    "\n",
    "            # If we reach here, the record should be included\n",
    "            # Extract all attributes as strings for consistency\n",
    "            row_data = {\n",
    "                k: str(v) if v is not None else None for k, v in elem.attrib.items()\n",
    "            }\n",
    "            all_data.append(row_data)\n",
    "\n",
    "            # Track all columns for consistent schema\n",
    "            ordered_columns.update(row_data.keys())\n",
    "            total_processed += 1\n",
    "\n",
    "            # Clear the element from memory\n",
    "            elem.clear()\n",
    "            while elem.getprevious() is not None:\n",
    "                del elem.getparent()[0]\n",
    "\n",
    "            # Process in batches\n",
    "            if len(all_data) >= batch_size:\n",
    "                if chunk_to_disk:\n",
    "                    # Create and save dataframe chunk\n",
    "                    chunk_df = pl.from_dicts(all_data, infer_schema_length=100000)\n",
    "                    chunk_file = f\"{temp_dir}/chunk_{chunk_count}.parquet\"\n",
    "                    chunk_df.write_parquet(chunk_file)\n",
    "                    chunk_files.append(chunk_file)\n",
    "                    chunk_count += 1\n",
    "                    all_data = []  # Free memory\n",
    "                else:\n",
    "                    # Keep accumulating in memory if not chunking to disk\n",
    "                    if total_processed % 10000 == 0:\n",
    "                        print(\n",
    "                            f\"Processed {total_processed} records, skipped {total_skipped}\"\n",
    "                        )\n",
    "\n",
    "                if total_processed % 10000 == 0:\n",
    "                    print(\n",
    "                        f\"Processed {total_processed} records, skipped {total_skipped}\"\n",
    "                    )\n",
    "\n",
    "    finally:\n",
    "        # Terminate the subprocess if it's still running\n",
    "        if process.poll() is None:\n",
    "            \n",
    "            process.terminate()\n",
    "\n",
    "    # Final results processing\n",
    "    print(f\"Completed. Total processed: {total_processed}, skipped: {total_skipped}\")\n",
    "\n",
    "    if chunk_to_disk and chunk_files:\n",
    "        # Combine all chunks from disk\n",
    "        dfs = [pl.read_parquet(file) for file in chunk_files]\n",
    "        if dfs:\n",
    "            # Combine all DataFrames\n",
    "            df = pl.concat(dfs, how=\"diagonal_relaxed\")\n",
    "            # Ensure all columns have same type\n",
    "            df = df.select([pl.col(col).cast(pl.String) for col in df.columns])\n",
    "            return df\n",
    "        else:\n",
    "            return pl.DataFrame()\n",
    "    else:\n",
    "        # Process in-memory data\n",
    "        if all_data:\n",
    "            df = pl.from_dicts(all_data, infer_schema_length=100000)\n",
    "            return df\n",
    "        else:\n",
    "            return pl.DataFrame()\n",
    "\n",
    "\n",
    "def process_stackoverflow_data(\n",
    "    archive_path,\n",
    "    output_file=\"stackoverflow_filtered_data.parquet\",\n",
    "    start_date=None,\n",
    "    end_date=None,\n",
    "    batch_size=5000,\n",
    "    large_file=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    High-level function to process StackOverflow XML data with non-empty titles.\n",
    "\n",
    "    Args:\n",
    "        archive_path (str): Path to the 7z archive containing XML data\n",
    "        output_file (str): Path to save the output parquet file\n",
    "        start_date (str): Optional start date filter in 'YYYY-MM-DD' format\n",
    "        end_date (str): Optional end date filter in 'YYYY-MM-DD' format\n",
    "        batch_size (int): Batch size for processing\n",
    "        large_file (bool): If True, use disk-based chunking for very large files\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: Processed data with non-empty titles\n",
    "    \"\"\"\n",
    "    print(f\"Processing {archive_path} for records with non-empty titles\")\n",
    "\n",
    "    # Process the file\n",
    "    df = process_xml_in_7z(\n",
    "        archive_path=archive_path,\n",
    "        batch_size=batch_size,\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "        chunk_to_disk=large_file,\n",
    "    )\n",
    "\n",
    "    if not df.is_empty():\n",
    "        print(f\"Found {len(df)} records with non-empty titles\")\n",
    "        print(f\"Columns: {df.columns}\")\n",
    "\n",
    "        # Save to parquet\n",
    "        df.write_parquet(output_file)\n",
    "        print(f\"Data saved to {output_file}\")\n",
    "    else:\n",
    "        print(\"No matching records found\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data/law.stackexchange.com.7z for records with non-empty titles\n",
      "Executing: 7z e -so data/law.stackexchange.com.7z Posts.xml\n",
      "Completed. Total processed: 11325, skipped: 62019\n",
      "Found 11325 records with non-empty titles\n",
      "Columns: ['Id', 'PostTypeId', 'CreationDate', 'Score', 'ViewCount', 'Body', 'OwnerUserId', 'LastEditorUserId', 'LastEditDate', 'LastActivityDate', 'Title', 'Tags', 'AnswerCount', 'CommentCount', 'ClosedDate', 'ContentLicense', 'LastEditorDisplayName', 'AcceptedAnswerId', 'OwnerDisplayName', 'FavoriteCount']\n",
      "Data saved to data/law/law.parquet\n"
     ]
    }
   ],
   "source": [
    "df_law = process_stackoverflow_data(\n",
    "    \"data/law.stackexchange.com.7z\",\n",
    "    output_file=\"data/law/law.parquet\",\n",
    "    batch_size=100000,\n",
    "    start_date=\"2021-01-01\",\n",
    "    end_date=\"2024-12-31\",\n",
    "    large_file=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-4a15aba03e6a468692122356f53fce1a.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-4a15aba03e6a468692122356f53fce1a.vega-embed details,\n",
       "  #altair-viz-4a15aba03e6a468692122356f53fce1a.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-4a15aba03e6a468692122356f53fce1a\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-4a15aba03e6a468692122356f53fce1a\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-4a15aba03e6a468692122356f53fce1a\");\n",
       "    }\n",
       "\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      let deps = [\"vega-embed\"];\n",
       "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-ff0c05433df0956bfebbabac2d64fa47\"}, \"mark\": {\"type\": \"line\", \"tooltip\": true}, \"encoding\": {\"x\": {\"field\": \"YM\", \"type\": \"nominal\"}, \"y\": {\"field\": \"Count\", \"type\": \"quantitative\"}}, \"params\": [{\"name\": \"param_1\", \"select\": {\"type\": \"interval\", \"encodings\": [\"x\", \"y\"]}, \"bind\": \"scales\"}], \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-ff0c05433df0956bfebbabac2d64fa47\": [{\"YM\": \"2021-01\", \"Count\": 306}, {\"YM\": \"2021-02\", \"Count\": 276}, {\"YM\": \"2021-03\", \"Count\": 303}, {\"YM\": \"2021-04\", \"Count\": 300}, {\"YM\": \"2021-05\", \"Count\": 265}, {\"YM\": \"2021-06\", \"Count\": 280}, {\"YM\": \"2021-07\", \"Count\": 339}, {\"YM\": \"2021-08\", \"Count\": 339}, {\"YM\": \"2021-09\", \"Count\": 319}, {\"YM\": \"2021-10\", \"Count\": 269}, {\"YM\": \"2021-11\", \"Count\": 275}, {\"YM\": \"2021-12\", \"Count\": 225}, {\"YM\": \"2022-01\", \"Count\": 290}, {\"YM\": \"2022-02\", \"Count\": 226}, {\"YM\": \"2022-03\", \"Count\": 263}, {\"YM\": \"2022-04\", \"Count\": 266}, {\"YM\": \"2022-05\", \"Count\": 320}, {\"YM\": \"2022-06\", \"Count\": 334}, {\"YM\": \"2022-07\", \"Count\": 335}, {\"YM\": \"2022-08\", \"Count\": 400}, {\"YM\": \"2022-09\", \"Count\": 358}, {\"YM\": \"2022-10\", \"Count\": 309}, {\"YM\": \"2022-11\", \"Count\": 299}, {\"YM\": \"2022-12\", \"Count\": 297}, {\"YM\": \"2023-01\", \"Count\": 345}, {\"YM\": \"2023-02\", \"Count\": 328}, {\"YM\": \"2023-03\", \"Count\": 369}, {\"YM\": \"2023-04\", \"Count\": 286}, {\"YM\": \"2023-05\", \"Count\": 238}, {\"YM\": \"2023-06\", \"Count\": 245}, {\"YM\": \"2023-07\", \"Count\": 237}, {\"YM\": \"2023-08\", \"Count\": 235}, {\"YM\": \"2023-09\", \"Count\": 249}, {\"YM\": \"2023-10\", \"Count\": 351}, {\"YM\": \"2023-11\", \"Count\": 261}, {\"YM\": \"2023-12\", \"Count\": 239}, {\"YM\": \"2024-01\", \"Count\": 282}, {\"YM\": \"2024-02\", \"Count\": 239}, {\"YM\": \"2024-03\", \"Count\": 228}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_plot = (\n",
    "    df_law.with_columns(\n",
    "        pl.col(\"CreationDate\").cast(pl.Datetime),\n",
    "        pl.col(\"LastActivityDate\").cast(pl.Datetime),\n",
    "    )\n",
    "    .with_columns(pl.col(\"CreationDate\").dt.strftime(\"%Y-%m\").alias(\"YM\"))\n",
    "    # .filter(pl.col(\"Tags\").str.contains(\"java\"))\n",
    "    .group_by(\"YM\")\n",
    "    .agg(pl.len().alias(\"Count\"))\n",
    "    .sort(\"YM\")\n",
    ")\n",
    "df_plot.plot.line(x=\"YM\", y=\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data/academia.stackexchange.com.7z for records with non-empty titles\n",
      "Executing: 7z e -so data/academia.stackexchange.com.7z Posts.xml\n",
      "Completed. Total processed: 9992, skipped: 138814\n",
      "Found 9992 records with non-empty titles\n",
      "Columns: ['Id', 'PostTypeId', 'CreationDate', 'Score', 'ViewCount', 'Body', 'OwnerUserId', 'LastActivityDate', 'Title', 'Tags', 'AnswerCount', 'CommentCount', 'ContentLicense', 'AcceptedAnswerId', 'LastEditorUserId', 'LastEditDate', 'ClosedDate', 'FavoriteCount', 'OwnerDisplayName', 'LastEditorDisplayName', 'CommunityOwnedDate']\n",
      "Data saved to data/academia/academia.parquet\n"
     ]
    }
   ],
   "source": [
    "df_ac = process_stackoverflow_data(\n",
    "    \"data/academia.stackexchange.com.7z\",\n",
    "    output_file=\"data/academia/academia.parquet\",\n",
    "    batch_size=100000,\n",
    "    start_date=\"2021-01-01\",\n",
    "    end_date=\"2024-12-31\",\n",
    "    large_file=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-0e000215143c452ea22870e47762034b.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-0e000215143c452ea22870e47762034b.vega-embed details,\n",
       "  #altair-viz-0e000215143c452ea22870e47762034b.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-0e000215143c452ea22870e47762034b\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-0e000215143c452ea22870e47762034b\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-0e000215143c452ea22870e47762034b\");\n",
       "    }\n",
       "\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      let deps = [\"vega-embed\"];\n",
       "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-662ad68abf8e713e495595eb3d6036b3\"}, \"mark\": {\"type\": \"line\", \"tooltip\": true}, \"encoding\": {\"x\": {\"field\": \"YM\", \"type\": \"nominal\"}, \"y\": {\"field\": \"Count\", \"type\": \"quantitative\"}}, \"params\": [{\"name\": \"param_3\", \"select\": {\"type\": \"interval\", \"encodings\": [\"x\", \"y\"]}, \"bind\": \"scales\"}], \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-662ad68abf8e713e495595eb3d6036b3\": [{\"YM\": \"2021-01\", \"Count\": 341}, {\"YM\": \"2021-02\", \"Count\": 282}, {\"YM\": \"2021-03\", \"Count\": 382}, {\"YM\": \"2021-04\", \"Count\": 294}, {\"YM\": \"2021-05\", \"Count\": 357}, {\"YM\": \"2021-06\", \"Count\": 296}, {\"YM\": \"2021-07\", \"Count\": 317}, {\"YM\": \"2021-08\", \"Count\": 307}, {\"YM\": \"2021-09\", \"Count\": 289}, {\"YM\": \"2021-10\", \"Count\": 327}, {\"YM\": \"2021-11\", \"Count\": 297}, {\"YM\": \"2021-12\", \"Count\": 271}, {\"YM\": \"2022-01\", \"Count\": 251}, {\"YM\": \"2022-02\", \"Count\": 238}, {\"YM\": \"2022-03\", \"Count\": 208}, {\"YM\": \"2022-04\", \"Count\": 208}, {\"YM\": \"2022-05\", \"Count\": 213}, {\"YM\": \"2022-06\", \"Count\": 195}, {\"YM\": \"2022-07\", \"Count\": 213}, {\"YM\": \"2022-08\", \"Count\": 216}, {\"YM\": \"2022-09\", \"Count\": 250}, {\"YM\": \"2022-10\", \"Count\": 222}, {\"YM\": \"2022-11\", \"Count\": 256}, {\"YM\": \"2022-12\", \"Count\": 218}, {\"YM\": \"2023-01\", \"Count\": 251}, {\"YM\": \"2023-02\", \"Count\": 211}, {\"YM\": \"2023-03\", \"Count\": 240}, {\"YM\": \"2023-04\", \"Count\": 246}, {\"YM\": \"2023-05\", \"Count\": 262}, {\"YM\": \"2023-06\", \"Count\": 226}, {\"YM\": \"2023-07\", \"Count\": 215}, {\"YM\": \"2023-08\", \"Count\": 262}, {\"YM\": \"2023-09\", \"Count\": 263}, {\"YM\": \"2023-10\", \"Count\": 257}, {\"YM\": \"2023-11\", \"Count\": 255}, {\"YM\": \"2023-12\", \"Count\": 208}, {\"YM\": \"2024-01\", \"Count\": 255}, {\"YM\": \"2024-02\", \"Count\": 181}, {\"YM\": \"2024-03\", \"Count\": 212}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_plot = (\n",
    "    df_ac.with_columns(\n",
    "        pl.col(\"CreationDate\").cast(pl.Datetime),\n",
    "        pl.col(\"LastActivityDate\").cast(pl.Datetime),\n",
    "    )\n",
    "    .with_columns(pl.col(\"CreationDate\").dt.strftime(\"%Y-%m\").alias(\"YM\"))\n",
    "    # .filter(pl.col(\"Tags\").str.contains(\"java\"))\n",
    "    .group_by(\"YM\")\n",
    "    .agg(pl.len().alias(\"Count\"))\n",
    "    .sort(\"YM\")\n",
    ")\n",
    "df_plot.plot.line(x=\"YM\", y=\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data/physics.stackexchange.com.7z for records with non-empty titles\n",
      "Executing: 7z e -so data/physics.stackexchange.com.7z Posts.xml\n",
      "Completed. Total processed: 62307, skipped: 514994\n",
      "Found 62307 records with non-empty titles\n",
      "Columns: ['Id', 'PostTypeId', 'AcceptedAnswerId', 'CreationDate', 'Score', 'ViewCount', 'Body', 'OwnerUserId', 'LastActivityDate', 'Title', 'Tags', 'AnswerCount', 'CommentCount', 'ContentLicense', 'LastEditorUserId', 'LastEditDate', 'ClosedDate', 'CommunityOwnedDate', 'OwnerDisplayName', 'FavoriteCount', 'LastEditorDisplayName']\n",
      "Data saved to data/physics/academia.parquet\n"
     ]
    }
   ],
   "source": [
    "df_ph = process_stackoverflow_data(\n",
    "    \"data/physics.stackexchange.com.7z\",\n",
    "    output_file=\"data/physics/physics.parquet\",\n",
    "    batch_size=100000,\n",
    "    start_date=\"2021-01-01\",\n",
    "    end_date=\"2024-12-31\",\n",
    "    large_file=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-5bd8ddc70bf14363b222a9b73fe3c2ca.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-5bd8ddc70bf14363b222a9b73fe3c2ca.vega-embed details,\n",
       "  #altair-viz-5bd8ddc70bf14363b222a9b73fe3c2ca.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-5bd8ddc70bf14363b222a9b73fe3c2ca\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-5bd8ddc70bf14363b222a9b73fe3c2ca\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-5bd8ddc70bf14363b222a9b73fe3c2ca\");\n",
       "    }\n",
       "\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      let deps = [\"vega-embed\"];\n",
       "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-025776561ab6cc1d4ab5ba0622f1a5b1\"}, \"mark\": {\"type\": \"line\", \"tooltip\": true}, \"encoding\": {\"x\": {\"field\": \"YM\", \"type\": \"nominal\"}, \"y\": {\"field\": \"Count\", \"type\": \"quantitative\"}}, \"params\": [{\"name\": \"param_4\", \"select\": {\"type\": \"interval\", \"encodings\": [\"x\", \"y\"]}, \"bind\": \"scales\"}], \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-025776561ab6cc1d4ab5ba0622f1a5b1\": [{\"YM\": \"2021-01\", \"Count\": 2230}, {\"YM\": \"2021-02\", \"Count\": 1923}, {\"YM\": \"2021-03\", \"Count\": 2269}, {\"YM\": \"2021-04\", \"Count\": 2154}, {\"YM\": \"2021-05\", \"Count\": 2283}, {\"YM\": \"2021-06\", \"Count\": 1963}, {\"YM\": \"2021-07\", \"Count\": 1732}, {\"YM\": \"2021-08\", \"Count\": 1789}, {\"YM\": \"2021-09\", \"Count\": 1770}, {\"YM\": \"2021-10\", \"Count\": 1754}, {\"YM\": \"2021-11\", \"Count\": 1674}, {\"YM\": \"2021-12\", \"Count\": 1626}, {\"YM\": \"2022-01\", \"Count\": 1791}, {\"YM\": \"2022-02\", \"Count\": 1597}, {\"YM\": \"2022-03\", \"Count\": 1489}, {\"YM\": \"2022-04\", \"Count\": 1528}, {\"YM\": \"2022-05\", \"Count\": 1681}, {\"YM\": \"2022-06\", \"Count\": 1592}, {\"YM\": \"2022-07\", \"Count\": 1427}, {\"YM\": \"2022-08\", \"Count\": 1500}, {\"YM\": \"2022-09\", \"Count\": 1423}, {\"YM\": \"2022-10\", \"Count\": 1441}, {\"YM\": \"2022-11\", \"Count\": 1371}, {\"YM\": \"2022-12\", \"Count\": 1332}, {\"YM\": \"2023-01\", \"Count\": 1683}, {\"YM\": \"2023-02\", \"Count\": 1235}, {\"YM\": \"2023-03\", \"Count\": 1355}, {\"YM\": \"2023-04\", \"Count\": 1446}, {\"YM\": \"2023-05\", \"Count\": 1481}, {\"YM\": \"2023-06\", \"Count\": 1325}, {\"YM\": \"2023-07\", \"Count\": 1401}, {\"YM\": \"2023-08\", \"Count\": 1485}, {\"YM\": \"2023-09\", \"Count\": 1366}, {\"YM\": \"2023-10\", \"Count\": 1373}, {\"YM\": \"2023-11\", \"Count\": 1367}, {\"YM\": \"2023-12\", \"Count\": 1279}, {\"YM\": \"2024-01\", \"Count\": 1396}, {\"YM\": \"2024-02\", \"Count\": 1288}, {\"YM\": \"2024-03\", \"Count\": 1488}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_plot = (\n",
    "    df_ph.with_columns(\n",
    "        pl.col(\"CreationDate\").cast(pl.Datetime),\n",
    "        pl.col(\"LastActivityDate\").cast(pl.Datetime),\n",
    "    )\n",
    "    .with_columns(pl.col(\"CreationDate\").dt.strftime(\"%Y-%m\").alias(\"YM\"))\n",
    "    # .filter(pl.col(\"Tags\").str.contains(\"java\"))\n",
    "    .group_by(\"YM\")\n",
    "    .agg(pl.len().alias(\"Count\"))\n",
    "    .sort(\"YM\")\n",
    ")\n",
    "df_plot.plot.line(x=\"YM\", y=\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data/math.stackexchange.com.7z for records with non-empty titles\n",
      "Executing: 7z e -so data/math.stackexchange.com.7z Posts.xml\n",
      "Processed 100000 records, skipped 3253522\n",
      "Processed 200000 records, skipped 3354510\n",
      "Processed 300000 records, skipped 3450219\n",
      "Completed. Total processed: 321580, skipped: 3470855\n",
      "Found 300000 records with non-empty titles\n",
      "Columns: ['Id', 'PostTypeId', 'AcceptedAnswerId', 'CreationDate', 'Score', 'ViewCount', 'Body', 'OwnerUserId', 'LastActivityDate', 'Title', 'Tags', 'AnswerCount', 'CommentCount', 'ContentLicense', 'LastEditorUserId', 'LastEditDate', 'ClosedDate', 'OwnerDisplayName', 'LastEditorDisplayName', 'FavoriteCount', 'CommunityOwnedDate']\n",
      "Data saved to data/math/math.parquet\n"
     ]
    }
   ],
   "source": [
    "df_ma = process_stackoverflow_data(\n",
    "    \"data/math.stackexchange.com.7z\",\n",
    "    output_file=\"data/math/math.parquet\",\n",
    "    batch_size=100000,\n",
    "    start_date=\"2021-01-01\",\n",
    "    end_date=\"2024-12-31\",\n",
    "    large_file=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-a61cdb989250480bb2d132edbfc6a17a.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-a61cdb989250480bb2d132edbfc6a17a.vega-embed details,\n",
       "  #altair-viz-a61cdb989250480bb2d132edbfc6a17a.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-a61cdb989250480bb2d132edbfc6a17a\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-a61cdb989250480bb2d132edbfc6a17a\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-a61cdb989250480bb2d132edbfc6a17a\");\n",
       "    }\n",
       "\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      let deps = [\"vega-embed\"];\n",
       "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-bf116ac9166531755a4e2fc5e0f9be8a\"}, \"mark\": {\"type\": \"line\", \"tooltip\": true}, \"encoding\": {\"x\": {\"field\": \"YM\", \"type\": \"nominal\"}, \"y\": {\"field\": \"Count\", \"type\": \"quantitative\"}}, \"params\": [{\"name\": \"param_5\", \"select\": {\"type\": \"interval\", \"encodings\": [\"x\", \"y\"]}, \"bind\": \"scales\"}], \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-bf116ac9166531755a4e2fc5e0f9be8a\": [{\"YM\": \"2021-01\", \"Count\": 13221}, {\"YM\": \"2021-02\", \"Count\": 12028}, {\"YM\": \"2021-03\", \"Count\": 13384}, {\"YM\": \"2021-04\", \"Count\": 12520}, {\"YM\": \"2021-05\", \"Count\": 11456}, {\"YM\": \"2021-06\", \"Count\": 9401}, {\"YM\": \"2021-07\", \"Count\": 8044}, {\"YM\": \"2021-08\", \"Count\": 8078}, {\"YM\": \"2021-09\", \"Count\": 8614}, {\"YM\": \"2021-10\", \"Count\": 9684}, {\"YM\": \"2021-11\", \"Count\": 9534}, {\"YM\": \"2021-12\", \"Count\": 8381}, {\"YM\": \"2022-01\", \"Count\": 8811}, {\"YM\": \"2022-02\", \"Count\": 7754}, {\"YM\": \"2022-03\", \"Count\": 8235}, {\"YM\": \"2022-04\", \"Count\": 7955}, {\"YM\": \"2022-05\", \"Count\": 8151}, {\"YM\": \"2022-06\", \"Count\": 7461}, {\"YM\": \"2022-07\", \"Count\": 6963}, {\"YM\": \"2022-08\", \"Count\": 6607}, {\"YM\": \"2022-09\", \"Count\": 6897}, {\"YM\": \"2022-10\", \"Count\": 8256}, {\"YM\": \"2022-11\", \"Count\": 7690}, {\"YM\": \"2022-12\", \"Count\": 6827}, {\"YM\": \"2023-01\", \"Count\": 7193}, {\"YM\": \"2023-02\", \"Count\": 6420}, {\"YM\": \"2023-03\", \"Count\": 7359}, {\"YM\": \"2023-04\", \"Count\": 7424}, {\"YM\": \"2023-05\", \"Count\": 8136}, {\"YM\": \"2023-06\", \"Count\": 7018}, {\"YM\": \"2023-07\", \"Count\": 6759}, {\"YM\": \"2023-08\", \"Count\": 6163}, {\"YM\": \"2023-09\", \"Count\": 6485}, {\"YM\": \"2023-10\", \"Count\": 7741}, {\"YM\": \"2023-11\", \"Count\": 7794}, {\"YM\": \"2023-12\", \"Count\": 5556}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_plot = (\n",
    "    df_ma.with_columns(\n",
    "        pl.col(\"CreationDate\").cast(pl.Datetime),\n",
    "        pl.col(\"LastActivityDate\").cast(pl.Datetime),\n",
    "    )\n",
    "    .with_columns(pl.col(\"CreationDate\").dt.strftime(\"%Y-%m\").alias(\"YM\"))\n",
    "    # .filter(pl.col(\"Tags\").str.contains(\"java\"))\n",
    "    .group_by(\"YM\")\n",
    "    .agg(pl.len().alias(\"Count\"))\n",
    "    .sort(\"YM\")\n",
    ")\n",
    "df_plot.plot.line(x=\"YM\", y=\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data/stackoverflow.com-Posts.7z for records with non-empty titles\n",
      "Executing: 7z e -so data/stackoverflow.com-Posts.7z Posts.xml\n",
      "Processed 100000 records, skipped 50941048\n",
      "Processed 200000 records, skipped 51071575\n",
      "Processed 300000 records, skipped 51199892\n",
      "Processed 400000 records, skipped 51327192\n",
      "Processed 500000 records, skipped 51455032\n",
      "Processed 600000 records, skipped 51581951\n",
      "Processed 700000 records, skipped 51711507\n",
      "Processed 800000 records, skipped 51841194\n",
      "Processed 900000 records, skipped 51969489\n",
      "Processed 1000000 records, skipped 52100047\n",
      "Processed 1100000 records, skipped 52232351\n",
      "Processed 1200000 records, skipped 52362887\n",
      "Processed 1300000 records, skipped 52492746\n",
      "Processed 1400000 records, skipped 52619123\n",
      "Processed 1500000 records, skipped 52746017\n",
      "Processed 1600000 records, skipped 52876885\n",
      "Processed 1700000 records, skipped 53005148\n",
      "Processed 1800000 records, skipped 53130551\n",
      "Processed 1900000 records, skipped 53254833\n",
      "Processed 2000000 records, skipped 53380705\n",
      "Processed 2100000 records, skipped 53507095\n",
      "Processed 2200000 records, skipped 53634524\n",
      "Processed 2300000 records, skipped 53763236\n",
      "Processed 2400000 records, skipped 53891675\n",
      "Processed 2500000 records, skipped 54024421\n",
      "Processed 2600000 records, skipped 54156445\n",
      "Processed 2700000 records, skipped 54285217\n",
      "Processed 2800000 records, skipped 54411588\n",
      "Processed 2900000 records, skipped 54539334\n",
      "Processed 3000000 records, skipped 54668900\n",
      "Processed 3100000 records, skipped 54797379\n",
      "Processed 3200000 records, skipped 54921552\n",
      "Processed 3300000 records, skipped 55021834\n",
      "Processed 3400000 records, skipped 55120124\n",
      "Processed 3500000 records, skipped 55219974\n",
      "Processed 3600000 records, skipped 55322733\n",
      "Processed 3700000 records, skipped 55425928\n",
      "Processed 3800000 records, skipped 55529023\n",
      "Processed 3900000 records, skipped 55633801\n",
      "Processed 4000000 records, skipped 55735583\n",
      "Completed. Total processed: 4043935, skipped: 55775113\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df = process_stackoverflow_data(\n",
    "    \"data/stackoverflow.com-Posts.7z\",\n",
    "    batch_size=100000,\n",
    "    start_date=\"2021-01-01\",\n",
    "    end_date=\"2024-12-31\",\n",
    "    large_file=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
