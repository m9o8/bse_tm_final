{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (2, 3)\n",
      "┌────────┬────────────┬─────────────┐\n",
      "│ period ┆ book_count ┆ total_words │\n",
      "│ ---    ┆ ---        ┆ ---         │\n",
      "│ str    ┆ u32        ┆ i64         │\n",
      "╞════════╪════════════╪═════════════╡\n",
      "│ low    ┆ 2          ┆ 148805      │\n",
      "│ high   ┆ 7          ┆ 1324014     │\n",
      "└────────┴────────────┴─────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Loading\n",
    "\n",
    "import polars as pl\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Function to read books and create initial dataframe\n",
    "def load_corpus(directory_path, period_label):\n",
    "    book_data = []\n",
    "    \n",
    "    # Get all text files in directory\n",
    "    for filepath in glob.glob(f\"{directory_path}/*.txt\"):\n",
    "        book_id = Path(filepath).stem\n",
    "        \n",
    "        # Read file in chunks to handle large files\n",
    "        with open(filepath, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            \n",
    "        book_data.append({\n",
    "            \"book_id\": book_id,\n",
    "            \"period\": period_label,\n",
    "            \"full_text\": text,\n",
    "            \"word_count\": len(text.split())\n",
    "        })\n",
    "    \n",
    "    # Create polars DataFrame\n",
    "    return pl.from_dicts(book_data)\n",
    "\n",
    "# Load books from both periods\n",
    "high_inequality_df = load_corpus(\"data/pre/\", \"high\")\n",
    "low_inequality_df = load_corpus(\"data/post/\", \"low\")\n",
    "\n",
    "# Combine into one DataFrame\n",
    "corpus_df = pl.concat([high_inequality_df, low_inequality_df])\n",
    "\n",
    "# Show summary statistics\n",
    "print(corpus_df.group_by(\"period\").agg(\n",
    "    pl.len().alias(\"book_count\"),\n",
    "    pl.sum(\"word_count\").alias(\"total_words\")\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/m9o8/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define economic vocabulary (expand this based on your research)\n",
    "economic_terms = {\n",
    "    \"wealth\": [\"wealth\", \"rich\", \"fortune\", \"affluent\", \"prosperity\", \"opulence\", \"luxury\"],\n",
    "    \"poverty\": [\"poverty\", \"poor\", \"destitute\", \"hunger\", \"squalor\", \"misery\", \"beggar\"],\n",
    "    \"upper_class\": [\"aristocrat\", \"nobleman\", \"gentleman\", \"lord\", \"lady\", \"elite\"],\n",
    "    \"lower_class\": [\"worker\", \"laborer\", \"servant\", \"peasant\", \"commoner\"]\n",
    "}\n",
    "\n",
    "# Flatten the terms list for easier searching\n",
    "all_terms = []\n",
    "term_categories = {}\n",
    "for category, terms in economic_terms.items():\n",
    "    for term in terms:\n",
    "        all_terms.append(term)\n",
    "        term_categories[term] = category\n",
    "\n",
    "# Function to extract sentences containing economic terms\n",
    "def extract_economic_sentences(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    economic_sentences = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence_lower = sentence.lower()\n",
    "        found_terms = []\n",
    "        \n",
    "        for term in all_terms:\n",
    "            # Use word boundary for exact match\n",
    "            pattern = r'\\b' + term + r'\\b'\n",
    "            if re.search(pattern, sentence_lower):\n",
    "                found_terms.append(term)\n",
    "        \n",
    "        if found_terms:\n",
    "            economic_sentences.append({\n",
    "                \"sentence\": sentence,\n",
    "                \"terms\": found_terms,\n",
    "                \"categories\": [term_categories[term] for term in found_terms]\n",
    "            })\n",
    "    \n",
    "    return economic_sentences\n",
    "\n",
    "# Apply extraction (warning: this can be memory intensive for large texts)\n",
    "def process_books(df):\n",
    "    # Process in smaller batches if needed\n",
    "    results = []\n",
    "    \n",
    "    for row in df.iter_rows(named=True):\n",
    "        book_sentences = extract_economic_sentences(row[\"full_text\"])\n",
    "        for sentence_data in book_sentences:\n",
    "            results.append({\n",
    "                \"book_id\": row[\"book_id\"],\n",
    "                \"period\": row[\"period\"],\n",
    "                \"sentence\": sentence_data[\"sentence\"],\n",
    "                \"terms\": sentence_data[\"terms\"],\n",
    "                \"categories\": sentence_data[\"categories\"]\n",
    "            })\n",
    "    \n",
    "    return pl.from_dicts(results)\n",
    "\n",
    "# Process in chunks to avoid memory issues\n",
    "sentence_df = process_books(corpus_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/m9o8/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "/tmp/ipykernel_13837/2590145157.py:25: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "  sentiment_df = sentence_df.with_columns([\n"
     ]
    }
   ],
   "source": [
    "# Calculate term frequencies by period and category\n",
    "term_freq_df = (sentence_df\n",
    "    .with_columns([\n",
    "        pl.col(\"terms\").list.len().alias(\"term_count\")\n",
    "    ])\n",
    "    .explode(\"terms\", \"categories\")\n",
    "    .group_by([\"period\", \"categories\", \"terms\"])\n",
    "    .agg([\n",
    "        pl.len().alias(\"frequency\")\n",
    "    ])\n",
    "    .sort([\"period\", \"categories\", \"frequency\"], descending=[False, False, True])\n",
    ")\n",
    "\n",
    "# Implement sentiment analysis on the sentences\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to get sentiment scores\n",
    "def get_sentiment(text):\n",
    "    scores = sid.polarity_scores(text)\n",
    "    return scores['compound']\n",
    "\n",
    "# Apply sentiment analysis with Polars\n",
    "sentiment_df = sentence_df.with_columns([\n",
    "    pl.col(\"sentence\").map_elements(get_sentiment).alias(\"sentiment_score\")\n",
    "])\n",
    "\n",
    "# Aggregate sentiment by period and category\n",
    "sentiment_by_category = (sentiment_df\n",
    "    .explode(\"categories\")\n",
    "    .group_by([\"period\", \"categories\"])\n",
    "    .agg([\n",
    "        pl.mean(\"sentiment_score\").alias(\"avg_sentiment\"),\n",
    "        pl.std(\"sentiment_score\").alias(\"std_sentiment\"),\n",
    "        pl.len().alias(\"sentence_count\")\n",
    "    ])\n",
    "    .sort([\"period\", \"categories\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high       0.50      1.00      0.67         1\n",
      "         low       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m9o8/documents/bse/trimester 2/textmining/bse_tm_final/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/m9o8/documents/bse/trimester 2/textmining/bse_tm_final/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/m9o8/documents/bse/trimester 2/textmining/bse_tm_final/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for classification\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Group sentences by book to get book-level features\n",
    "book_sentences = (sentence_df\n",
    "    .group_by([\"book_id\", \"period\"])\n",
    "    .agg([\n",
    "        pl.col(\"sentence\").implode().flatten().alias(\"sentences\")\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Join all sentences from each book\n",
    "book_sentences = book_sentences.with_columns([\n",
    "    pl.col(\"sentences\").list.join(\" \").alias(\"text\")\n",
    "])\n",
    "\n",
    "# Convert to Python lists for sklearn\n",
    "texts = book_sentences[\"text\"].to_list()\n",
    "labels = book_sentences[\"period\"].to_list()\n",
    "\n",
    "# Create TF-IDF features\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 2),\n",
    "    stop_words='english'\n",
    ")\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Get most important features\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "feature_importance = clf.feature_importances_\n",
    "features_df = pl.DataFrame({\n",
    "    \"feature\": feature_names,\n",
    "    \"importance\": feature_importance\n",
    "}).sort(\"importance\", descending=True).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot term frequency differences between periods\n",
    "def plot_term_differences():\n",
    "    # Pivot the data\n",
    "    plot_data = (term_freq_df\n",
    "        .pivot(\n",
    "            values=\"frequency\",\n",
    "            index=[\"terms\", \"categories\"],\n",
    "            on=\"period\"\n",
    "        )\n",
    "        .with_columns([\n",
    "            (pl.col(\"high\") - pl.col(\"low\")).alias(\"difference\")\n",
    "        ])\n",
    "        .sort(\"difference\", descending=True)\n",
    "    )\n",
    "    \n",
    "    # Extract top 15 terms for plotting\n",
    "    top_terms = plot_data.head(15)\n",
    "    \n",
    "    # Create plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Convert to Pandas for easier plotting (Polars plotting still developing)\n",
    "    pd_data = top_terms.to_pandas().set_index(\"terms\")\n",
    "    \n",
    "    # Plot\n",
    "    pd_data[[\"high\", \"low\"]].plot(kind=\"barh\", ax=plt.gca())\n",
    "    plt.title(\"Top 15 Economic Terms by Period Difference\")\n",
    "    plt.xlabel(\"Frequency\")\n",
    "    plt.ylabel(\"Terms\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"imgs/term_frequency_comparison.svg\")\n",
    "    plt.close()\n",
    "\n",
    "# Plot sentiment analysis results\n",
    "def plot_sentiment_analysis():\n",
    "    # Convert to Pandas for plotting\n",
    "    pd_data = sentiment_by_category.to_pandas()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Create grouped bar chart\n",
    "    sns.barplot(\n",
    "        x=\"categories\", \n",
    "        y=\"avg_sentiment\", \n",
    "        hue=\"period\",\n",
    "        data=pd_data\n",
    "    )\n",
    "    \n",
    "    plt.title(\"Average Sentiment by Economic Category and Period\")\n",
    "    plt.xlabel(\"Economic Category\")\n",
    "    plt.ylabel(\"Average Sentiment Score\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"imgs/sentiment_by_category.svg\")\n",
    "    plt.close()\n",
    "\n",
    "# Create visualizations\n",
    "plot_term_differences()\n",
    "plot_sentiment_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings for Economic Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import spacy\n",
    "\n",
    "# Load SpaCy for better tokenization and parsing\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\"])\n",
    "\n",
    "nlp.max_length = 4000000\n",
    "\n",
    "def preprocess_for_embeddings(text):\n",
    "    \"\"\"Process text for embedding training, preserving sentence boundaries\"\"\"\n",
    "    doc = nlp(text)\n",
    "    sentences = []\n",
    "    for sent in doc.sents:\n",
    "        # Keep only content words, convert to lowercase\n",
    "        tokens = [token.lemma_.lower() for token in sent \n",
    "                 if not token.is_stop and not token.is_punct and token.is_alpha]\n",
    "        if tokens:\n",
    "            sentences.append(tokens)\n",
    "    return sentences\n",
    "\n",
    "# Process books and train period-specific embeddings\n",
    "def train_period_embeddings(df, period):\n",
    "    all_sentences = []\n",
    "    \n",
    "    # Process each book\n",
    "    for row in df.filter(pl.col(\"period\") == period).iter_rows(named=True):\n",
    "        # Process in chunks for very large texts\n",
    "        processed = preprocess_for_embeddings(row[\"full_text\"])\n",
    "        all_sentences.extend(processed)\n",
    "    \n",
    "    # Train Word2Vec model on this period's corpus\n",
    "    model = Word2Vec(sentences=all_sentences, vector_size=100, window=5, min_count=5, workers=4)\n",
    "    model.save(f\"embeddings_{period}.model\")\n",
    "    return model\n",
    "\n",
    "# Train models for each period\n",
    "high_inequality_model = train_period_embeddings(corpus_df, \"high\")\n",
    "low_inequality_model = train_period_embeddings(corpus_df, \"low\")\n",
    "\n",
    "# Find economic concept spaces through semantic similarity\n",
    "def explore_economic_concepts(model, seed_terms=[\"rich\", \"poor\", \"money\", \"wealth\"]):\n",
    "    \"\"\"Expand economic vocabulary through embeddings\"\"\"\n",
    "    economic_space = {}\n",
    "    for term in seed_terms:\n",
    "        if term in model.wv:\n",
    "            similar_terms = model.wv.most_similar(term, topn=20)\n",
    "            economic_space[term] = similar_terms\n",
    "    return economic_space\n",
    "\n",
    "# Compare economic vocabularies between periods\n",
    "high_economic_space = explore_economic_concepts(high_inequality_model)\n",
    "low_economic_space = explore_economic_concepts(low_inequality_model)\n",
    "\n",
    "# Find words with largest shifts in economic associations\n",
    "def compare_word_associations(word, model1, model2, topn=10):\n",
    "    \"\"\"Compare how a word's associations differ between periods\"\"\"\n",
    "    if word not in model1.wv or word not in model2.wv:\n",
    "        return None\n",
    "    \n",
    "    assoc1 = set([w for w, _ in model1.wv.most_similar(word, topn=topn)])\n",
    "    assoc2 = set([w for w, _ in model2.wv.most_similar(word, topn=topn)])\n",
    "    \n",
    "    return {\n",
    "        \"unique_to_model1\": list(assoc1 - assoc2),\n",
    "        \"unique_to_model2\": list(assoc2 - assoc1),\n",
    "        \"common\": list(assoc1 & assoc2)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# Extract sentences by book\n",
    "def get_book_sentences(df):\n",
    "    books_data = []\n",
    "    for row in df.iter_rows(named=True):\n",
    "        doc = nlp(row[\"full_text\"])\n",
    "        sentences = [sent.text for sent in doc.sents]\n",
    "        books_data.append({\n",
    "            \"book_id\": row[\"book_id\"],\n",
    "            \"period\": row[\"period\"],\n",
    "            \"sentences\": sentences\n",
    "        })\n",
    "    return pl.from_dicts(books_data)\n",
    "\n",
    "# Preprocess for topic modeling\n",
    "def preprocess_for_topics(sentences):\n",
    "    processed_docs = []\n",
    "    for sent in sentences:\n",
    "        doc = nlp(sent)\n",
    "        # Keep lemmatized forms of content words\n",
    "        tokens = [token.lemma_.lower() for token in doc \n",
    "                 if not token.is_stop and not token.is_punct \n",
    "                 and token.is_alpha and len(token.text) > 2]\n",
    "        if tokens:\n",
    "            processed_docs.append(tokens)\n",
    "    return processed_docs\n",
    "\n",
    "# Apply topic modeling to find economic themes\n",
    "def extract_economic_topics(book_sentences_df, num_topics=10):\n",
    "    all_docs = []\n",
    "    book_ids = []\n",
    "    periods = []\n",
    "    \n",
    "    for row in book_sentences_df.iter_rows(named=True):\n",
    "        processed = preprocess_for_topics(row[\"sentences\"])\n",
    "        all_docs.extend(processed)\n",
    "        book_ids.extend([row[\"book_id\"]] * len(processed))\n",
    "        periods.extend([row[\"period\"]] * len(processed))\n",
    "    \n",
    "    # Create dictionary and corpus\n",
    "    dictionary = corpora.Dictionary(all_docs)\n",
    "    dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in all_docs]\n",
    "    \n",
    "    # Train LDA model\n",
    "    lda_model = LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=dictionary,\n",
    "        num_topics=num_topics,\n",
    "        passes=10,\n",
    "        alpha='auto',\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Get dominant topic for each document\n",
    "    doc_topics = []\n",
    "    for i, doc_bow in enumerate(corpus):\n",
    "        top_topics = lda_model.get_document_topics(doc_bow, minimum_probability=0.0)\n",
    "        top_topic = max(top_topics, key=lambda x: x[1])\n",
    "        \n",
    "        doc_topics.append({\n",
    "            \"book_id\": book_ids[i],\n",
    "            \"period\": periods[i],\n",
    "            \"top_topic_id\": top_topic[0],\n",
    "            \"top_topic_prob\": top_topic[1],\n",
    "            \"all_topics\": dict(top_topics)\n",
    "        })\n",
    "    \n",
    "    #doc_topics_df = pl.from_dicts(doc_topics)\n",
    "    \n",
    "    # Extract topic words for interpretation\n",
    "    topics_words = {}\n",
    "    for topic_id in range(num_topics):\n",
    "        topics_words[topic_id] = [word for word, prob in lda_model.show_topic(topic_id, topn=20)]\n",
    "    \n",
    "    return doc_topics, topics_words, lda_model, corpus, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topics, topic_words, lda_model, corpus, dictionary = extract_economic_topics(get_book_sentences(corpus_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
